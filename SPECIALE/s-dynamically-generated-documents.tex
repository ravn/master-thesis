% $Id$


\chapter{Dynamically generated documents}
\label{cha:dynamically-generated-documents}

% \framepage{10cm}{Abbreviations are explained in ``Terms and
%   Concepts'' (chapter \myvref{cha:terms-and-concepts}).  Please refer to
%   it to clarify matters.}

\mycitation{404 Document not found}{}{}


\section{The way it started: Handmade web-pages in a
  filesystem on a web-server}

Back in the old days when the world wide web was designed
(1989), a web server could basically do only two things:



\begin{itemize}
\item Serve \textit{static} files directly from an underlying file
  system.  These files were either pages written in HTML, or images in
  GIF or JPEG formats.

\item Call a program complying to the \textit{Common Gateway
    Interface} with user-submitted parameters, and return its output
  as the document to send.
\end{itemize}

Nothing else!

\mydetour{

Such simple web servers still exist - for example
\myurl{http://hoohoo.ncsa.uiuc.edu/}{the NCSA server} (which has been
unmaintained since 1996 -- NCSA recommend that the Apache server
should be used instead).  Even so
\myurl{http://www.netcraft.com/Survey/Reports/0002/}{the February 2000
  Netcraft survey} reported that 17172 NCSA servers were active, and
could be reached from Netcraft.

In August 1995 the number of
\myurl{http://www.netcraft.com/Survey/Reports/9508/ALL/}{webservers
  running the NSCA server was 10835}, meaning that number has
increased slightly in that period.  For comparison the total number of
public webservers on the Internet in the same period grew from 19
thousand to 11 million, with Apache and Microsoft Internet Information
Server accounting for 8.8 million of these.
  
}


Even with such a simple model, it works well for even \textit{very}
large web-sites which only generates few documents dynamically.

A well-tuned server which serves documents directly from the
filesystem can reach impressive numbers.
\myurl{http://www.acme.com/software/thttpd/benchmarks.html}{ACME
  software has a webserver benchmark} from 1998, with which they have
benchmarked several webservers, where Apache did not come out top.


On a modern PC the Apache webserver has no problem saturating a 10
Mbps Ethernet connection, meaning that even for very large and
demanding sites the bottleneck should be determined by the hardware!
Phillip Greenspun talks about databases with added webserver
capability in \cite{phillipandalexsguidetowebpublishing}, and conclude
that these are generally much slower than their non-database
relatives.

On a related note: The ftp-server \texttt{ftp.cdrom.com} serves a
\textit{lot} of files every day.  On March 14 2000
\myurl{http://www.emsphone.com/stats/cdrom.html}{the transfer
  statistics} said that the average output that day was 79.3 Mbit/s
(with a peak of 95.8 Mbit/s).  The average output on a yearly basis
was 86.2 Mbit/s (which is 933 Gb/day in average).
\myurl{ftp://ftp.cdrom.com/.message}{This single machine runs FreeBSD}
(see \myurl{http://www.freebsd.org}{www.freebsd.org} for details).
Since an ftp-session has a notably larger overhead than an
http-session (since an ftp-session includes that the \texttt{ftp}-user
does a login to the system), it is not unreasonable to expect similar
performance from a suitably tuned web-server serving static files.


\subsection{CGI-scripts}
\label{sec:cgi-scripts}



Calling CGI-scripts is another matter.  For the occasional dynamically
generated document CGI-scripts turned out to work well.  All kinds of
documents -- images, webpages, progress reports -- could be generated
comparatively easy in the favorit language of the CGI-script author.
Libraries to deal with the decoding of parameters, and encoding of the
generated result were soon abundant, providing for many enhancements
to the original ``static web site'' model.

\myimage{gr/xerox-1}{The original Xerox PARC Map Viewer}{xerox-1}
\label{sec:map-generation-first-web-application} One of the first
demonstrations of dynamically generated images was the
\myurl{http://mapweb.parc.xerox.com/map}{Xerox PARC Map Viewer} (see
figure~\myvref{fig:xerox-1}) which allowed the user to navigate a
virtual atlas, where each ``window'' to the map was generated on
demand.  This initial quick hack has since been superseded by
professional map companies which produce maps of an uneven quality on
demand, like \myurl{http://maps.yahoo.com/py/maps.py}{Yahoo Maps} (USA
only) and \myurl{http://www.mapquest.com}{MapQuest} (covers Europe
too).


% \textsf{a reference to the problems with CGI?  From CGI.pm perhaps}

There is several problems with CGI-scripts:

\begin{itemize}
\item \textbf{Scripts run as subprocesses} -- the script is executed
  with the same permissions as the webserver itself, including access
  to \texttt{/etc/passwd} and other possibly sensitive files.  Such
  scripts had to be \textit{trusted} or - for Internet Service
  Providers - inspected before installation to ensure that the script
  would behave properly.
  
  This has been addressed with the development of \textit{safe
    languages} where the execution environment is guaranteed that the
  CGI-script is confined to a restricted area where it cannot do any
    damage (a so-called \textit{sandbox}).

\item \textbf{Incompatible platforms} -- the CGI-programmer may not
  have access to a C-compiler which can generate binary code for the
  machine running the webserver.

  This has caused interpreted languages like Perl to be very popular.
  Perl programs can be run immediately on a given platform without
  needing to be recompiled, and are for most purposes as fast as the
  equivalent programs written in \texttt{C} or \texttt{C++},
  especially for users with slow modems.
  
  Recently Java Servlets (see section~\myvref{sec:java-servlets}) have
  appeared as a popular and well-supported alternative to CGI-scripts,
  but requires that the hosting web-server is capable of running a
  Java Virtual Machine.  Servlets are being supported by more and more
  web servers.

\item \textbf{Slow} -- the overhead just for invoking the CGI-program
  in a subprocess is substantial even for moderate load on the web
  server, since the web-server must ``fork'' a new process for each
  request.

  This has been addressed by putting the execution environment inside
  the web server, using threads instead of processes, and caching
  compiled versions of scripts.

\end{itemize}

Several solutions to the above problems have emerged.  Of these, Java
Servlets are discussed in section~\myvref{sec:CGI-servlets}, PHP3
scripts in section~\myvref{sec:CGI-php3}, and Perl (with the mod\_perl
acceleration module) in section~\myvref{sec:CGI-modperl}).

Server side scripting never really took off before the PHP3 and ASP
languages provided \textit{persistent connections} to database
servers, which was unavailable to the standard CGI-scripts.

Persistent connections provide a real performance boost!  Database
connections are notoriously expensive in time and processing power to
establish, so needing to open one for each and every CGI-script were a
true performance bottleneck.  Just by keeping the connection open (and
for ASP-scripts - retain the particular database connection for the
session) was extremely beneficial.  That combined with a very simple
way to switch between code and HTML plus that every user could use it
without needing to ask the webmaster to install a potentially
dangerous CGI-script, meant that the ability to generate webpages
dynamically suddenly became available to everyone.


\section{When a site grows, it becomes hard to maintain}
\label{sec:when-a-site-grows-it-becomes-hard-to-maintain}

With the growth of the Internet many webmasters find that their site
has grown way beyond what they originally expected, and that

\begin{itemize}
\item their handwritten HTML pages are hard to keep up-to-date.  A
  minor change in site layout, must be manually updated in all the
  affected files.  Creating scripts to do this helps, but is still a
  non-trivial task
\item The information tree described by the links is very difficult to
  change, since all links must be checked and potentially changed.
\item When pages are moved or deleted, it becomes difficult to ensure
  that all links pointing to these pages are dealt with appropriately.
  \myurl{http://www.useit.com/alertbox/980614.html}{Jakob Nielsen}
  uses the term \textit{linkrot} to describe web sites with many
  broken links.
\item In the case of external links it becomes even harder to ensure
  that all links are valid at all times.
\end{itemize}

The answer to these problems is usually \textit{automation}.  A script
to ensure that the  ``Last updated 2000-03-12 by whoever'' string at
the bottom of each page is up to date.   A script to ensure that all
internal links are valid.  A script to update the layout of tables.
The list is endless.

\section{Navigational structure should not be maintained by the author}

\myimage{gr/flug-1}{The Fyns GNU/Linux User Group web-site.  The side
  bar and the logo was added automatically by a script by
  Tobias Bardino.}{flug-1}

One of the easiest things to automate for a webmaster is the insertion
of a few snippets of HTML in each web page to give a characteristic
site layout.  This relieves the webmaster of the tedious task of
updating all the HTML-documents on the site manually.

Figure~\myvref{fig:flug-1} show a sample page from the
\myurl{http://www.flug.dk}{Fyns GNU/Linux User Group web-site},
showing the way FLUG chose to do it.  The framework shown is added to
each HTML-file with a small perl script.

This is a typical static framework for a site, where every web page
basically is ``inserted'' in the home page since the set of links is
identical for all pages.  Usually this involves a link to a search
engine for the site.

In the original web servers this required that every web page was
modified to include the snippet of HTML which produced this header,
which is a relatively easy task with a modern scripting language like
Perl.  Note that care must be taken not to change the modification
times of the files, since failing to do so invalidates local copies in
browsers and webcaches, even though that the contents of the pages is
unchanged as such.  (Modern webservers allow a webpage to include
other files with
\myurl{http://www.apache.org/docs/mod/mod\_include.html}{Server Side
  Includes}, which takes care of all this - except doing it
unconditionally on every page.  The user must still remember to insert
the appropriate command sequence in the server).

\myimage{gr/intel-1}{The web page for drivers for the Intel VS440FX
  motherboard}{intel-1}%
%
Many modern websites put a given page in a \textit{context} where the
navigational framework reflect this context.  See
figure\myvref{fig:intel-1} for a sample from
\myurl{http://support.intel.com/support/motherboards/desktop/VS440fx/software.htm}{Intel
  regarding the VS440FX motherboard}, where the actual web page has a
top bar with generic links, a column with links specific to
motherboards in general, and a review form at the bottom asking
whether this information was useful to the reader (this question is
asked on every page with technical content).  My personal experience
with the Intel site is that their search engine is efficient, but that
the link column is too uninformative - you often have to actually
follow the link to see what is there.  An expanded view would be nice.

This site layout require a bit more discipline for the web author
writing each page, but is still easily implementable since it can
still be implemented by including the appropriate HTML-snippet, as
these are the same for all pages regarding this motherboard.  A way to
easily ensure this, would be to let the name of the physical file
determine the virtual location in the web server hierachi - the shown
file could have been named
\texttt{motherboards/vs440fx/drivers.html}.  This may be difficult to
manage (if the webmasters work on different machines) and it is not
very easy to change if the global layout of the web server changes.

In order to ensure integrity in the navigational framework for a site
this size, it should be generated automatically.  Doing so requires
meta-data about the individual pages in order to place them correctly
in the framework.

\section{A good website needs meta-information about its documents}

What to a webmaster is a nice and well maintained web-site, is to a
computer just a bunch of directories of files with bytes in them.  In
order to get any use of a computer in maintaining these, it is
important to have easily accessible information about the desired
functionality and the files to work with.  Even though complicated
rules can be constructed to extract information from webpages, the
basic rule is still that

\begin{center}
  \fbox{\textit{A human must enter the basic
  information for categorizing a given webdocument}. }
\end{center}

Computers are simply not good enough yet to guess this information
themselves.

Incidentially this is also the reason why Internet search engines like
\myurl{http://www.altavista.com}{AltaVista} needs elaborate
information extraction techniques in order to remain useful.
AltaVista was the first Internet Search engine that claimed to cover
\textit{all} webpages on the Internet.  It was immediately a great
success since the Internet had already grown to a size where help was
essential to find any page if you didn't have a direct URL to it
already.

It didn't take long for web authors (especially those with adult
material) to realize that the best way to get their web pages
frequently returned in a top position at AltaVista was by putting as
many potential search terms in META-tags in each and every of their
web pages, showing that just blindly extracting keywords
\textit{uncrititically} is too simple a method.  Additionally this
drives users away -- when they feel that they are not getting ``the
best results'' from the search engine they will look for another one
that can. \myurl{http://www.google.com}{The Google search engine} (see
section~\myvref{sec:google}) was such an engine, and users quickly
started to use it. Recently links to Amazon.com and Fatbrain.com have
begun to crop up in the top of the search results on Google, implying
that Google has started to sell search results (where a given link is
returned with a much better score than warranted by the search term
the user provided), and as a direct result users are going elsewhere.

As an alternative to Google I have been recommended
\myurl{http://www.alltheweb.com}{AllTheWeb}, but which after a bit of
use appears to be prone to the same polluting documents as AltaVista.
See the result of ``background icons'' in
figure~\myvref{fig:alltheweb-2} and what the fifteenth hit actually
linked to in figure~\myvref{fig:alltheweb-1}. This link was
chosen because the site \textit{expects} the user to have
arrived from a search on these two terms.


\myimage{gr/alltheweb-2}{AllTheWeb search for ``background
  icons''.}{alltheweb-2}

\myimage{gr/alltheweb-1}{And what hit 15 was linking to
  in~\myvref{fig:alltheweb-2}.  This link was chosen
  because the site in question openly admit that they expect
  search engines to show this story when users search for
  ``beautiful icons'', even though that the story is about sex.  
  }{alltheweb-1}


AltaVista failed then as AllTheWeb fails now, because they had no
control over authors and the authors had a desire for ``breaking the
system''.  For further reading, Douglas Hofstadter talks a lot about
the impossibility of building an unbreakable system for automatic
detection of bad input in~\cite{goedelescherbach}.



\myimage{gr/mip-1}{The MIP Recently Changed Pages.  The last
  of the system pages and the first user pages are
  shown.}{mip-1} An example of a document generated from
meta-data which has been automatically extracted, is the MIP
``Recently Changed Pages'' (see figure~\myvref{fig:mip-1}) the
original version of which I wrote while working for MIP.
The script traverses three separate set of web pages on the
server - System pages, System Administrator pages and User
pages (one set per user) - and generates a list for each set
sorted by title.  Each file listed has its age in days next
to it.

The meta-information extracted was:

\begin{itemize}
\item \textbf{Title} - used for the link, and sorting the entries in a
  set.  Extracted from the content of each document, with a default of
  the filename if no title was present.
\item \textbf{Age} - extracted from the underlying filesystem which
  registers the last change of the document.
\item \textbf{User} (for the user pages) - also extracted from the
  filesystem.  It is also used to look up the picture of the user.

%\item \textbf{Size} - which is intended to provide a hint to the user
%  about the size of the page in question.   It is not used in the
%  depicted page.

\end{itemize}

The flat Unix-filesystem can only provide one thing more,
namely the size of the file, and nothing at all is
guaranteed about the contents of a HTML-file - even the
title is not even always there!

If more meta-data than the above is needed, the authors must be
involved and as a part of their web-authoring, deliberately and
carefully ensure that the meta-information needed by the automatic
processes is correct and up-to-date.

Such an effort requires that the data is much more systematically
organized than they are today, including better mark-up.  I have not
seen any site on the Internet yet, which is capable of
\textit{categorizing} the content of the Internet.

The \textit{Yggdrasil} system (see
section~\myvref{sec:yggdrasil}) was my first attempt to
generate a navigational framework from information extracted
from webpages.  Yggdrasil was to function as the automatic
webmaster on the intranet, in order to avoid having to
assign staff to do so.  Then the individual employee could
publish information in form of a webpage, add a category
either in the title or as a META-tag, and trigger the next
update of the framework.  This update would scan all
web-pages, and extract tuples of (author, category code,
publishing date, title, size) of those web-pages which had a
category code, and generate a tree structure of web-pages to
navigate the categories.  Additionally lists of ``Documents
sorted by author'' and ``Documents sorted by date'' were
generated to help users locate documents.

Yggdrasil was used on a closed Intranet without external
access and the users were interested in using this as a
tool.  The incentive for ``breaking the system'' was very
low, and the tool worked well, especially when you take the
small amount of meta-information available in consideration.


\myimage{gr/yahoo-1}{The British Yahoo site.}{yahoo-1}

An example of a good site built with meta-information is
\myurl{http://www.yahoo.com}{Yahoo} (see
figure~\myvref{fig:yahoo-1}) which started as a directory over
web pages where the maintainers added meta-data to a lot of
web-sites by categorizing them manually, and use this
information to regularily generate static navigational
pages.  At a time Yahoo really suffered by a lot of broken
links as the meta-data was not maintained, but today Yahoo
usually links to a page that \textit{is} there (by verifying
that a target page existed).


\section{Documentation - meta-data of programs}

\mycitation{Program documentation is notoriously poor, and maintenance
  is worse.  Changes made in the program do not promptly, accurately,
  and invariably appear in the [separate paper documentation].  The
  solution, I think, is to merge the files, to incorporate the
  documentation in the source program.  This is at once a powerful
  incentive toward proper maintenance, and an insurance that the
  documentation will always be handy to the program user.  Such
  programs are called
  \textit{self-documenting}}{\cite{brooks:the-mythical-man-month}}{1975}

Given there is a need for meta-data about the documents, the question
is then \textit{where} should the authors put the meta-data?


In the programming world, a very visible example of meta-data is the
documentation for programs, since the computers themselves do not need
them.  Experience has shown that it is notoriously hard for
programmers to keep the documentation up-to-date, since it is usually
considered a part of the coding process that is not really necessary
and definitively not felt to be a part of the ``creative, fun''
process of writing programs!  If the writing of documentation was well
established as an integrated part of program development, and the
programming language and the programmers working environment itself
helped as much as it possibly could, it would be easier for the
programmers to keep the documentation synchronized with the code.

The quote above (from \cite{brooks:the-mythical-man-month}) show that
the thought that documentation should be as close to the thing it
documents as possible isn't new.  For programs, this means that the
documentation should be \textit{in the same file} as the code.
Comments are usually just that -- brief comments -- and are often
meant to explain \textit{what} the code does instead of \textit{why}?
The abstraction level is not high enough in the traditional comments.

Andrew S. Tanenbaum writes excellent books about Computer Science, and
his Operating Systems book \cite{tanenbaumoperatingsystems} contains
the complete source listing with line numbers of his Minix operating
system, along with a cross-reference of all identifiers.  That was the
best the printing industry could do in 1987 (and is typical for
several other printed versions of source code), and that was not good
enough for teaching.  It is very hard to get a grasp of what the code
does, without long and careful studies, and much flapping back and
forth.  My guess is that today Tanenbaum would write a hyperlinked
book readable in a browser, perhaps even using Literate
Programming techniques (these are discussed in
section~\myvref{sec:literate-programming}).  The only other publicized
works of large programs I know of are the books by Knuth about {\TeX}
and the PGP 5.0 source code (which was not printed for humans to read,
but in order to \myurl{http://zone.pspt.fi/pgp/faq/pgp55scan.txt}{scan
  the PGP source to circumvent the USA crypto export regulations}).

The great news is that the industry recognizes the
usefulness of documentation on the web, and the need for
programmers themselves to create such documentation.  In
order to gain wide acceptance such meta-data management
systems must be \textit{standards}, either as \textit{de
  facto standards} or defined by a well-known organization
like ANSI or W3C.  No such standard has arisen yet, but an
XML-dialect will most likely be written for the purpose.


\subsection{Case study: JavaDoc - embedding web-information in programs}
\label{sec:javadoc}

\myimage{gr/acme-1}{The ACME documentation of the
  \texttt{Acme.Phase} class generated by JavaDoc}{acme-1}

This is perhaps the most visible and generally available meta-data
tool for Java programmers today.
\myurl{http://java.sun.com/products/jdk/1.2/docs/tooldocs/solaris/javadoc.html}{JavaDoc}
is a tool that creates documentation in form of HTML pages from Java
source code with embedded comments, where all definitions are parsed
and hyperlinked to give a full overview of the Java source code in
question.  See figure~\myvref{fig:acme-1} for a JavaDoc rendered source
code at \myurl{http://www.acme.com}{ACME labs.}

JavaDoc is a specialized tool which is only suitable for generating
web-pages from Java source code, but as Sun
\myurl{http://java.sun.com/products/jdk/1.2/docs/api/overview-summary.html}{use
  JavaDoc for their reference documentation} as well as ship it with
every copy of Java Development Kit, it is a tool which is widely
available.  Java programmers who need to document their code, will most
likely use JavaDoc to do so.

JavaDoc has also raised the expectations of the programmers, since
they have become used to hyper-linked documentation in a browser to
accompany any code they want to use.  

Others recognize this too.  On March 13, 2000
\myurl{http://relativity.yi.org/WebSite/opensource-javadoc/}{an
  open letter pleading for the release of JavaDoc as
  OpenSource} was posted to the Internet.  They argued
amongst other things that a number of bugs needed to be
fixed, and new techology like XML/XSL should be employed
too.  Hopefully this will influence Sun to help Java
developers as much as possible by opening up this source
too.

Meanwhile, \myurl{http://xml.apache.org/cocoon/javadoc.html}{an
  Apache effort to parse JavaDoc documents into XML} is underway.
This has great potential when it gets running, since the dynamic
publishing abilities of XML will complement the target audience of
  JavaDoc, namely programmers who need reference documentation in an
  electronic form.

\subsection{Case study: The Plain Old Documentation format for Perl}
\label{sec:perlpod}

\begin{figure}[htbp]
  \begin{center}
\lstinputlisting[language=Perl,frame=trbl]{sample.pod}
\caption{A sample POD-file}
    \label{fig:sample-pod-file}
  \end{center}
\end{figure}

\myimage{gr/pod-sample-1}{The HTML generated by
  \texttt{pod2html} looks like this}{pod-sample-1}

The
\myurl{http://www.cpan.org/doc/manual/html/pod/perlpod.html}{perlpod}
format was designed to be simple to write and easy to use in
Perl programs, and the overwhelming amount of freely
available \myurl{http://www.cpan.org}{documentation for
  Perl} confirms that this goal was reached.  This format
allows you -- with very elementary mark up -- to put
documentation and code in the same file, and you may leave
out either one.  Figure~\myvref{fig:sample-pod-file} is the
actual POD-file (marked up by \LaTeX) and
figure~\myvref{fig:pod-sample-1} show the HTML-version as
rendered by a browser.



All the info-files at
\myurl{http://www.unixsnedkeren.dk}{Unixsnedkeren.dk} (the
website for my small firm) have been written as POD files,
and converted to HTML, with some finishing touches done with
another Perlscript which has been evolved from
\myurl{http://www.fido.dk/faq/unix-faq/unix\_r23.htm}{several
  previous projects}.  Code truly lives forever, even though
it is very difficult to locate the start and end tags of a
given HTML-construction without parsing the whole file.

Due to this and all the rest of the \textit{ad hoc} code in both
\unixcommand{pod2html} and my own code, I am convinced that another
approach to rendering POD-files should be taken, and I have submitted
patches to the \unixcommand{pod2docbook} command in order to create
DocBook XML in addition to the current DocBook SGML.  This will
add the rendering possibilities of the XSL-world to those Perl already
know about.

Recently a \texttt{Lip::Pod} filter have been added to CPAN which
allows a mixed mode documentation output -- where both code
and documentation is output allowing a simple form of \textit{Literate
Programming}.

%\textsf{Code rearrangement?}



\subsection{Literate Programming - Knuths approach to different views
  of the source}
\label{sec:literate-programming}

\mycitation{I believe that the time is ripe for significantly better
  documentation of programs, and that we can best achieve this by
  considering programs to be works of literature. Hence, my title:
  "Literate Programming." Let us change our traditional attitude to
  the construction of programs: Instead of imagining that our main
  task is to instruct a *computer* what to do, let us concentrate
  rather on explaining to *human beings* what we want a computer to
  do.}%
{Donald Knuth}%
{\cite{knuthliterateprogramming} quoted from
  \myurl{http://www.literateprogramming.com/}{The Literate Programming
    web site}}

Donald E. Knuth has written the {\TeX}-system used to typeset this
report, and documented it by publishing the source code to the entire
system in four books.  The {\TeX}-system has impressed since it is of
an extremely high quality, both in code and documentation, and
he \myurl{http://truetex.com/knuthchk.htm}{actually offers money to those
  who find errors in his code}.

In order to write both documentation and code of such high quality,
Knuth developed his own method of coding which he named ``Literate
Programming'', in which the author works with a single file containing
the documentation \textit{as it is to be presented to the
  reader}.  Knuth uses a {\TeX}-dialect, in which he
explains his solution to a given problem, and all code is
provided as small \textit{chunks} with a name.   All chunks
with the same name are concatenated in a single chunk, which
is then inserted whenever that name is used.

This file format is the \texttt{web}-format!  This way of
programming is discussed in detail
in~\cite{sewell:weaving-a-program}, and a lot of references
are available on
\myurl{http://www.literateprograming.org}{literateprograming.org}.
A web-program can be processed in two ways:

\begin{itemize}
\item
The \texttt{weave} tool converts the web-file to a \texttt{tex}-file
to be processed by {\TeX}, and printed.

\item The \texttt{tangle} tool generates the actual code to be
compiled, by joining chunks with the same name, and inserting them in
other chunks that reference them (a simple macro expansion),
eventually producing one or more flat files which can be compiled,
along with information links each code chunk back to the original web document.

\end{itemize}

\begin{verbatim}

              sample.web
                /   \
               /     \
           weave    tangle
             /         \
            /           \
        sample.tex    a.c b.c ...
           |             |
           |             |
         ....         compiler
                         |
                         |
                      executable
\end{verbatim}

The problem with Knuth is that things that work well for him, does not
always work quite as well for the rest of us.  No editors - not even
Emacs - are currently capable of helping where a given file is
separated in a lot of logical regions, being either {\TeX}-code or
source code, meaning that the editor cannot provide the supporting
functions which a programmer might have become accustomed to.

You cannot code verbatim - some characters are reserved for other
purposes and must be written differently, which may be very annoying
to learn.  Additionally the concept of chunks all over the document
may be counter-productive if these are hard to navigate.

Many people have experimented with the possibilities of a weave/tangle
pair resulting in several software packages, some of which are
\myurl{http://www.ross.net/funnelweb/}{FunnelWeb},
\myurl{http://www.eecs.harvard.edu/~nr/noweb/}{noweb},
\myurl{http://w3.pppl.gov/~krommes/fweb.html\#SEC3}{fweb} and
\myurl{http://www.tu-chemnitz.de/~ukn/UNIX/nuwebdoc/nuwebdoc.html}{nuweb}.

Experiments with nuweb showed that it was language
independent (therefore well suited to Perl), working
reasonably well, and easy to integrate with {\TeX} and Emacs
using AUC-\TeX.  The major problem was adding code to make
AUC-\TeX aware that the \texttt{.tex}-file was generated
from another file.

There is
\myurl{http://www.webring.org/cgi-bin/webring?ring=litprog;list}{a
  webring for Literate Programming}, which is an excellent place to
look for further information.  The Collection of Computer Science
Bibliographies provides
\myurl{http://liinwww.ira.uka.de/searchbib/SE/litprog}{a search engine
  in literate programming publications}.

Oasis has \myurl{http://www.oasis-open.org/cover/xmlLitProg.html}{a
  page on Literate Programing with XML and SGML} where a number of
approaches are listed with references.  This appears to be the only
resource so far.

My personal experiences with Literate Programs can be summarized as:

\begin{itemize}
\item The documentation gets bigger and better, simply because the
  printed documents look better that way.  What would pass as a single
  line comment in a source file, looks very small when typeset.  The
  full power of {\TeX} also encourages usages of illustrations and
  graphs.

\item The program development gets cumbersome.  You may have trouble
  using your favorite tools for editing, compiling and debugging.
  Users of most integrated development environments cannot use this
  model since the IDE does not provide hooks to provide this
  processing.

\item A critical point is whether the intermediate files are visible
  to the author! In order to be usable, \textit{all} derived files
  \textit{must} refer to the original document in a transparent
  fashion, meaning that the user should not have to worry about
  intermediate files.  (In the same way that the C-processor works
  under Unix).
\end{itemize}

My conclusion was that the Literate Programming paradigm does not pay
off as a individual programmer, but may work very well for a larger
programming team where good, current documentation is critical.

\subsection{Rational Rose - the other way around}
\label{sec:rational-rose}

A software product which has been very successful in recent years, is
the \myurl{http://www.rational.com/products/rose/index.jtmpl}{Rational
  Rose visual modeling tool} which is used by several people at MIP to
do software development.

Rational Rose provides for a lot of things relevant to a large
software project like reverse engineering of existing code, but also
for
\myurl{http://www.rational.com/sitewide/support/whitepapers/dynamic.jtmpl?doc\_key=350}{several
  views of the document} depending on what abstraction level the
authors are working on.  In this way they are able to provide the
author an environment where the software model can be designed
seperately from the writing of the code.

Design patterns are used to provide commonly known building blocks to
build up the abstractions.

Bo N{\o}rreg{\aa}rd J{\o}rgensen, Ph.D. student at MIP, told
me that he would expect it to be beneficial to use Rational
Rose in a software project when it has an underlying model
and has 7 modules or more, where a module is a unit of code
that serves as a building block for the physical structure
of a system.
                       

\subsection{Compiling source code into an executable}
\label{sec:compiling-source-into-an-executable}

Most software projects known to me do not consist of a single huge
source file which would take a long time to compile, but of several smaller
files, which can be compiled individually and finally linked to an executable
file.  If a file with source code is changed, it is not necessary to
recompile each and every file but only those which depend on this
particular source file, and then relink the executable to incorporate
the changes.

This is possible because the authors has provided meta-data about the
system, regarding which source files the system contains, which
commands to call to compile and link the source files, which libraries
should be included, etc.  Normally an Integrated Development
Environment (like \myurl{http://www.borland.com/bcppbuilder/}{Borland
  C++Builder}, \myurl{http://msdn.microsoft.com/visualc/}{Microsoft
  Visual C++}, or \myurl{http://www-4.ibm.com/software/ad/}{the Visual
  Age products from IBM}) knows about these things, or a
\myurl{http://www.eng.hawaii.edu/Tutor/Make/}{Makefile} is constructed
to utilize the known relations built into \unixcommand{make} as well
as allow the author to add new ones.

The real advantage comes in environments where several compilations
can run in parallel due to multi-threaded and/or multi-CPU systems.
This can be done safely by using meta-data to localize compilations
which are independent from one another and execute these
simultaneously.  MIP used to have a 24-CPU Silicon Graphics machine
where a parallelizing ``make'' could reduce compilation times with a
factor of 20 or more.  Not all software projects were easily
parallelized - if the Makefile did not list all dependencies
explicitly but expected that the normal compilation order would
produce the unlisted files, the parallelized compilation would fail.



To summarize, \textit{adding meta-data to the project allow the program
development process to go faster}.


\section{Multiple views of a web document}
\label{sec:multiple-views-of-a-web-document}

\myimage{gr/zdnet-1}{The ZDNet presentation of a story --
  notice how little of the initial screen that is dedicated
  to the story about ``Microsoft may try to boost WinCE,
  Linux-style''.  The ``Printer Friendly version'' icon is
  about half way down the page from the start which is below
  the part shown.}{zdnet-1} \myimage{gr/zdnet-2}{The
  ``Printer-friendly version'' of
  figure~\myvref{fig:zdnet-1}}{zdnet-2}

Some web-sites have so many advertisements and auxiliary links in
their layout, that they need an additional version which is
well-suited for printing (i.e. only has a single ad, and is
rendered without margins in a fixed font, or just with a
minimal set of HTML-tags used).  See
figure~\myvref{fig:zdnet-1}
\myurl{http://www.zdnet.com/zdnn/stories/news/0,4586,2468874,00.html?chkpt=zdhpnews01}{for
  an example from ZDNet}, and figure~\myvref{fig:zdnet-2} for the
``Printer-friendly version''.

It is interesting to notice that websites realize that they cannot
just offer an advertisement ladden service, without allowing for a
reasonable paper version.  Since their HTML cannot ``remove'' the ads
for printing, they have to offer two different version of each
article.

\myimage{gr/nokia-1}{Browsing the Internet on the large [sic] display
  of a Nokia 9110.  Nokia do not state the dimensions of the
  display}{nokia-1}

These are two different \textit{views} of the same basic document.
ZDNet are expecting their visitors to either see their documents on a
modern display unit with high resolution and millions of colors
rendered by a reasonably capable browser or print them out on paper.
That will probably change within a few years when it becomes common
for other devices than just traditional computers to be connected to
the Internet.  Mobile phones using WAP do not have as much screen real
estate (the Nokia 9110 is shown in figure~\myvref{fig:nokia-1}) so such
users will probably prefer short and crisp versions of the documents
when they use a mobile phone.  Nokia is working with 3com to produce a
hybrid between a Palm Pilot and a mobile phone.

\mydetour{
  
This tendency might as a side-effect be beneficial for the currently
rather overlooked blind computer users (see
\myurl{http://www.webring.org/cgi-bin/webring?ring=blind\&list}{the
  Blind Web-ring} for more information) .  They must normally use
text-to-braille software or a ``screen reader'' in order to use
computers, neither of which work well with web pages loaded with
graphics, but when web authors must write for many kinds of media
instead of just a particular version of a given browser, this will
also be beneficial to these users since it will be easy to write a
renderer for their preferred format.  Of course, it is possible to
write ordinary HTML in a way that is usable by these users, but that
is rare these days.
}

Both \myurl{http://www.useit.com/alertbox/990613.html}{Jakob Nielsen}
and
\myurl{http://www.si.dk/netsteder/publ/tilgaeng/index.html}{Statens
  Information (in Danish)} have good guidelines for making pages with
web content accessible to people with disabilities based on
\myurl{http://www.w3.org/TR/WAI-WEBCONTENT/}{the work done by the W3C
  Web Accessory Initiative}.  Pages written with these guidelines in
mind will work -- not only for disabled readers -- but should be ready
for the technological advances in the near future.

\section{SGML/XML:  Generic formats for storing data and meta-data}
\label{sec:sgml-and-xml-generic-formats}

The problem of representing content in a generic way is not
new at all, and was solved for the printing industry in the
1970'ies by the design of the Standard Generalized Markup
Language (SGML) which is widely used.  HTML and XML are both
SGML-dialects albeit with different capabilities.  They are
discussed in chapter~\myvref{cha:sgml-xml-and-dtd's}.  A
\textit{Document Type Definition} (DTD) specifies the rules
that a given document must obey, in order to be compliant
with that DTD.

For now it is sufficient to say that it is possible to store both data
and meta-data in a convenient form in an SGML or XML file, but that
these files must be rendered into the formats expected by the users,
like HTML, PDF or what else tomorrow may bring of new, exiting
formats.  Since SGML describes the \textit{content} and not the
layout, a new output format is ``just'' a matter of creating a
renderer for any new format and add it to the collection.

This has been recognized by almost all of the ``Documentation
Projects'' (\myurl{http://www.linuxdoc.org/}{The Linux Documentation
  Project}, \myurl{http://www.freebsd.org/docproj/docproj.html}{The
  FreeBSD Documentation Project} and others) accompanying the various
free operating systems available on the Internet.


Chapter~\myvref{cha:on-demand-rendering} talks about rendering documents
to a given format on demand (for example in a web browser).



% Introduce publishing where HTML is just a backend amongst many (PDF,
% Word, ASCII).

% Describe why it is important to be able to render finished versions of
% documents fully automatically from a single \textsl{annotated} source
% (www/print/cdrom/Palm Pilot/braille/handicapped persons,whatever).  The better
% the annotation, the better the output (ref: Stibo).  Describe the need
% for SGML (history/usage) and XML (why/browser support/on-the-fly
% publishing/bleeding edge - being standardized).

% Donald Knuth - {\TeX}/Web/Literate Programming - advantages (high
% quality, excellent math, superb algorithms, can be tailored to needs
% (basic interpreter written in TeX) and
% disadvantages (programming language, not abstract) (designed 20 years ago).  Ask Steffen Enni
% about his thoughts.  Javadoc.  Perl POD.  DocBook projects (FreeBSD,
% LDP).


\section{The user should use familiar tools to publish documents}
\label{sec:the-user-should-use-familiar-tools-to-publish-documents}

Editing SGML manually is hard and tedious - this is one
thing that is generally agreed upon in the
\myurl{news:comp.text.sgml}{\texttt{comp.text.sgml}} and
\myurl{news:comp.text.xml}{\texttt{comp.text.xml}}
newsgroups, and quite sophisticated tools are essential to
make authoring directly in SGML viable.

Very few tools exist, and the good ones are quite expensive.  The
general consensus is that the best OpenSource tool is the Emacs editor
with the PSGML package (see~\myvref{sec:emacs-with-psgml}), and that
this tool is primarily suited for programmers and other people who are
well acquainted with SGML and XML.

Computers should \textit{help} you do your work, and users are
generally most productive with the tools they know.  Why should an
author use SGML with an editor she dislikes, if almost the same result
can be achieved by using a word processor she is familiar with?

Automated conversions from one document format to another is a
non-trivial matter, in particular when the target format contains more
meta-data than the original.  It is therefore important that the
author is fully aware of the underlying processes and use only those
constructions that the conversion program understands.  This is most
likely given as a set of macros that must be used, as well as a
verification program which the author can use at any time to check
whether her document is complying to the DTD.  These rules must be
strictly enforced during the writing process in order for the author
to be as productive as possible.

In addition to this, the publishing process should be as simple as
possible.  For comparison is it quite a complicated task to print a
document.  Numerous variables regarding choice of printer driver,
location of physical device, transporting the bytes across the cables,
and communicating with the printer to ensure that all is well during
printing.  This complicated task is today as simple as pressing a
``Print'' button and checking a few things in a dialogue box. (This is
the same approach Adobe uses in their Acrobat software -- they capture
the output to a printer and convert it directly to PDF).

Web publishing is not any more complicated or difficult than that, and
this should be reflected in the working procedure.  My conclusion is:

\begin{center}
  \textit{Publishing a document to the web, ought to be just
    as easy as printing a document.}
\end{center}

This conclusion, combined with my experiences with the Yggdrasil
system, have been one of several design goal in the Cactus system
which is described in chapter~\myvref{cha:cactus}, and which has been
fulfilled.



\section{Small search engines for a web site}
\label{sec:search-engines}

\textsf{Remember something about this}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "rapport"
%%% End:
