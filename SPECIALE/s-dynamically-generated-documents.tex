% $Id$


\chapter{Dynamically generated documents}
\label{cha:dynamically-generated-documents}

% \framepage{10cm}{Abbreviations are explained in ``Terms and
%   Concepts'' (chapter \vref{cha:terms-and-concepts}).  Please refer to
%   it to clarify matters.}

!!! PUT SAMPLE OF FILENAME INSTEAD OF TT-FILENAME-/TT in SOMEWHERE.


\section{The way it started: Handmade web-pages in a filesystem on a web-server}

\textsf{Badly phrased ...}  The original capabilities of a web server
was : \textsf{Nice figure... Mention URL}

\begin{itemize}
\item A underlying file system containing a lot of \textit{static}
files.  These files were either pages written in HTML, or images in
GIF or JPEG formats.

\item The \textit{Common Gateway Interface}.  If a given script
complied to this, the web-server could start it with user-provided
parameters, and return the result as any kind of file.
\end{itemize}

That's all.  

Such web servers still exist - for example
\myurl{http://hoohoo.ncsa.uiuc.edu/}{the NCSA server} which has been
unmaintained since 1996 (NCSA recommend that the Apache server should
be used instead).  Even so
\myurl{http://www.netcraft.com/Survey/Reports/0002/}{the February 2000
  Netcraft survey} reported that \textsf{17172} NCSA servers were
active, and could be reached from Netcraft.

In August 1995 the number of
\myurl{http://www.netcraft.com/Survey/Reports/9508/ALL/}{webservers
  running NSCA was 10835}, meaning that number has increased slightly
in that period.  For comparison the total number of public webservers
on the Internet in the same period grew from 19 thousand to 11
million, with Apache and Microsoft Internet Information Server
accounting for 8.8 million of these.

Even with such a simple model, it works well for even \textit{very}
large web-sites which only generates few documents dynamically.

A well-tuned server which serves documents directly from the
filesystem can reach impressive numbers.  \textsf{Where was this -
look for statistics}.  On a modern PC Apache has no problem saturating
a 10Mbps Ethernet connection (\textsf{what about a 100Mbps
connection)}, meaning that even for very large and demanding sites the
bottleneck will not be within the webserver software.

On a related note: The ftp-server \texttt{ftp.cdrom.com} serves a lot
of files everyday.  On March 14'th 2000
\myurl{http://www.emsphone.com/stats/cdrom.html}{the transfer
  statistics} said that the average output was 79.3 Mbit/s (with a
peak of 95.8 Mbit/s).  The average output on a yearly basis was 86.2
Mbit/s, which is 933 Gb/day in average).
\myurl{ftp://ftp.cdrom.com/.message}{This lone machine runs FreeBSD}
(see \myurl{http://www.freebsd.org}{www.freebsd.org} for details).  A
ftp-session have a notably larger overhead than a http-session, so it
is not unreasonable to expect similar performance from a suitably
tuned web-server for static files.

For the occasional dynamically generated document CGI-scripts turned
out to work well.  All kinds of documents - images, webpages, progress
reports - could be generated comparatively easy in the favorit
language of the CGI-script author.  Libraries to deal with the
decoding of parameters, and encoding of the generated result, were
soon abundant, providing for many enhancements to the original
``static web site''.

\label{sec:map-generation-first-web-application} One of the first
demonstrations of dynamically generated images was the \textsf{map
generator somewhere on the web (this ??? quick hack has since been
superceeded by professional map companies which produce high quality
maps on demand)}.


\textsf{a reference to the problems with CGI?  From CGI.pm perhaps}

Unfortunately, there are a few short-comings with CGI-scripts:

\begin{itemize}
\item \textbf{Runs as a subprocess} -- the script is executed with the
  same permissions as the webserver itself, including access to
  \texttt{/etc/passwd} and other possibly sensitive files.  Such
  scripts had to be \textit{trusted} or - for ISP's - inspected before
  installation to ensure that the script would behave properly.

  This has been addressed with the development of ``safe languages''
  where the execution environment is guaranteed that the CGI-script is
  confined to a ``sandbox''.
  
\item \textbf{Incompatible platforms} -- the CGI-programmer may not
  have access to a C-compiler which can generate binary code for the
  webserver.

  This has caused interpreted programs like Perl to be very popular.  These do
  not require recompilation for a given platform, and are for most
  purposes as fast as the equivalent programs written in C.   Recently
  Java servlets has shown as a popular and well-supported alternative
  to CGI-scripts.
  
\item \textbf{Slow} -- the overhead just for invoking the CGI-program
  in a subprocess is substantial even for moderate load on the web
  server, since the web-server must ``fork'' a new process.

  This has been addressed by putting the execution environment inside
  the web server, using threads instead of processes, and caching
  compiled versions of scripts.  

\end{itemize}

Several solutions to the above problems have emerged.  Java Servlets
are discussed in section~\vref{sec:CGI-servlets},  PHP3 scripts in
section~\vref{sec:CGI-php3}, and Perl (with the mod\_perl
acceleration module) in section~\vref{sec:CGI-modperl}).

\section{Navigational structure should not be maintained by the author}

\textsf{reference?}  Almost all dynamic web-sites which add and delete
web pages run into the problem of maintaining site-integrity,
regarding ensuring that all the ``links'' point to the correct
document.    For external document all you can do is to check
occasionally that the page is still there, but for internal documents
the webmaster has to do the updates manually.  Updating HTML-documents
manually is at best a tedious pain, because it is hard, repetitive,
mindless labour, which should be left to a computer.

\myimage{gr/flug-1}{The Fyns GNU/Linux User Group web-site.  The side
  bar and the top graphic was added
  automatically}{flug-1}

Figure~\vref{fig:flug-1} shows a sample page from the Fyns GNU/Linux
User Group web-site, showing the way FLUG chose to do it.  The
framework is added to the HTML-file with a small perl script which
must be run to publish each page.

\textsf{Does it hang together?}.
The most simple version of this, is to say that every web page on the
site must have a static header usually containing at least a link to the entry
page and a search engine, but where the header is the same for every
page.

In the original webservers this required that every webpage was
modified to include the snippet of HTML which produced this header,
which is a relatively easy task with a modern scripting language like
Perl.  Note that care must be taken not to change the modification times of the
files, since failing to do so invalidates local copies in browsers
and webcaches, even though that the contents of the pages \textsf{is}
unchanged.  (Modern webservers allow a webpage to include other files
with \textsf{Server Side Includes}, which takes care of all this - except
doing it unconditionally on every page.  The user must still remember
to insert the appropriate command sequence in the server).

\myimage{gr/intel-1}{The web page for drivers for the Intel VS440FX
  motherboard}{intel-1}%
%
Many modern websites put a given page in a
\textit{context} where the navigational framework reflect this
context.  See figure\vref{fig:intel-1} for a sample from
\myurl{http://support.intel.com/support/motherboards/desktop/VS440fx/software.htm}{Intel
  regarding the VS440FX motherboard}.  This requires more discipline
for the web author writing each page, but can still be implemented by
including the appropriate HTML-snippet, since these are the same for
all pages regarding this motherboard.

\textsf{Reference to INTEL.COM with navigational fraework for
  motherboard vs440fx}.

In order to ensure integrity in the navigational framework for a site
this site, it should be created automatically.  Doing so requires
meta-data about the individual pages in order to place them correct in
the framework.

\section{A good website needs meta-information about its documents}

What to a webmaster is a nice, and well maintained web-site, is to a
computer just a bunch of directories of files with bytes in them.  In
order to get any use of a computer in maintaining these, it is
important to have easily accessible information about the desired
functionality and the files to work with.  Even though complicated
rules can be constructed to extract information from webpages, the
basic rule is still that

\begin{center}
  \fbox{\textit{A human must enter the basic
  information for categorizing a given webdocument}. }
\end{center}



\textsf{check spelling of altavista}
Incidentially this is also the reason why Internet search engines like
needs elaborate information extraction techniques in order
to remain useful.  AltaVista (which is discussed in
section~\vref{sec:alta-vista}) was the first Internet Search engine
for \textit{all} webpages. \textsf{Check precise way to do it and
  expand the text a bit.  Web pages were considered in isolation or
  compared with the rest of the site?}

It didn't take long for web authors (especially those with adult
material) to realize that the best way to get their web pages
frequently returned in a top position at AltaVista was by putting as
many potential search terms in META-tags in each and every of their
web pages, showing that just blindly extracting keywords
\textsf{uncrititically} is too simple a method.  The Google search
engine (see~\vref{sec:google}) uses \textsf{heuristics} to assert the
quality of the results, and is - in my opinion - the only usable
search engine for the net today.

The search engines failed because they had no control over authors and
the authors had a desire for ``breaking the system''.  For further
reading, Douglas Hofstaedter talks a lot about the impossibility of
building an unbreakable system for automatic detection of bad input
in~\cite{goedelescherbach}.


\textsf{Screen shot of webpage here}

An example of a document generated from meta-data which has been
automatically extracted, is the MIP ``Recently Changed Pages'' (see
figure~\vref{fig:mip-recently-changed-pages}) the original version of
which I wrote while working for MIP as a student programmer.  The
script traverses three disjunkt set of web pages on the server -
System pages, Sysop pages and user pages (one set per user) - and
generates a list for each set sorted by title.  Each file listed has
its age in days next to it.

The meta-information extracted was:

\begin{itemize}
\item \textbf{Title} - used for the link, and sorting the entries in a
  set.  Extracted from the content of each document, with a default of
  the filename if no title was present.
\item \textbf{Age} - extracted from the underlying filesystem which
  registers the last change of the document
\item \textbf{User} (for the user pages) - also extracted from the
  filesystem (\textsf{or was it from the URL?}).  It is also used to
  look up the picture of the user.
\item \textbf{Size} - \textsf{Did I use it?  For anything?  Check code}
\end{itemize}

To me this is just about all the useful information a flat
Unix-filesystem can provide.  Additionally nothing is guaranteed about
the contents of a HTML-file - even the title is not even always
there, making it unreliable to rely on authors providing the
information expected by any automatic process.

If more meta-data than the above is needed, the authors must be
involved and as a part of their web-authoring, deliberately and
carefully ensure that the meta-information needed by the automatic
processes is correct and up-to-date.

The \textit{Yggdrasil system} (see section~\vref{sec:yggdrasil}) was
my first attempt to generate a navigational framework from information
extracted from webpages.  Yggdrasil was to function as the automatic
webmaster on the intranet, in order to avoid having to assign staff to
do so.  Then the individual employee could publish information in form
of a webpage, add a category either in the title or as a META-tag, and
trigger the next update of the framework.  This update would scan all
web-pages, and extract tuples of (author, category code, publishing
date, title, size) of those web-pages which had a category code, and
generate a tree structure of web-pages to navigate the categories.
Additionally lists of ``Documents sorted by author'' and ``Documents
sorted by date'' were generated to help users locate documents.

This was on a closed Intranet, where the users were interested in
using this as a tool.  The incentive for ``breaking the system'' was
very low.

Yggdrasil worked very well initially especially when its very low
amount of meta-information is taken in consideration.

\textsf{Screen shot from Yahoo} \textsf{vref{fig}}An example of a good
site built with meta-information is
\myurl{http://www.yahoo.com}{Yahoo} which started as a directory over
web pages where the maintainers manually categorized \textit{many} web
sites, and used this information to regularily generate static
navigational pages .  This works really well - I often use Yahoo as an
electronic librarian, allowing me to find a website on a given topic I
have never seen before (and Google to find it again at a later date) - 

The question is then \textit{where} should the author put the meta-data?

\section{Avoid chaos - keep information in its place}

In the programming world, a very visible example of meta-data is the
documentation for programs.  Experience has shown that it is
notoriously hard for programmers to keep the documentation up-to-date,
since it is usually considered a \textsf{get good quote from somewhere
  :-)}, which is not felt a part of the ``creative, fun'' process of
writing programs.  If the writing of documentation was well
established as an integrated part of program development, and the
programming language itself helped as much as it could, it would be
much easier for the programmers to keep the documentation updated.

Experience has shown (\textsf{references} man month) that
documentation should be as close to the thing it documents as
possible.  For programs, that means that the documentation should be
\textit{in the same file} as the code.  This has traditionally been
done by using comments embedded in the source code.

\framepage{15cm}{\textsf{grow terse - does not stand on its own - hard
    to maintain even for the author - abstraction layer: the idea was
    to \textit{hide} the code and just show the meta-data}}

\textsf{Typical example is Tanenbaum Minix with source listing with
  line numbers and a cross reference - bladre from og tilbage hele
  tiden.  1991 technology?  Designed for teaching.  Well written
  books, but horrible programmer \cite{tanenbaumoperatingsystems}}.

This observation is not new.  \textsf{New is that industry recognizes
  the usefulness of documentation on the web, and the need for
  programmers themselves to create such documentation.  In order to
  gain wide acceptance such meta-data management systems must be
  \textit{standards}, either as \textit{de-facto standards} or as a
  standard body.  }

\subsection{Javadoc - embedding web-information in programs}
\label{sec:javadoc}

\textsf{Get a screen shot}

This is perhaps the most visible meta-data management tool available
to programmers today.  \textsf{JavaDoc} is a tool that creates
documentation in form of HTML pages from Java source code with
embedded comments, \textsf{where all definitions are parsed to give a
  full overview of the Java source code}.

Javadoc is a specialized tool which is only suitable for generating
web-pages from Java source code, but as Sun both use it for their
reference documentation (\textsf{url to that}) as well as ship it with
every copy of Java Development Kit, it is a tool which is widely
available.  Programmers with a need to document their code, will most
likely use javadoc to do so.  \textsf{link to the java code at
ACME.com}

Javadoc has also raised the expectations of the programmers, since
they have become used to hyper-linked documentation in a browser to
accompany any code they are to use.  This tendency is a good step in
the right direction.


\subsection{The Plain Old Documentation format for Perl}
\label{sec:perlpod}

\textsf{list an example and say that this works well.  Just look at CPAN}

\subsection{Literate Programming - Knuths approach \textsf{??}}
\label{sec:literate-programming}

\textsf{THIS MIGHT HAVE TO BE MOVED}

\mycitation{I believe that the time is ripe for significantly better
  documentation of programs, and that we can best achieve this by
  considering programs to be works of literature. Hence, my title:
  "Literate Programming." Let us change our traditional attitude to
  the construction of programs: Instead of imagining that our main
  task is to instruct a *computer* what to do, let us concentrate
  rather on explaining to *human beings* what we want a computer to
  do.}%
{Donald Knuth}%
{\cite{knuthliterateprogramming} quoted from
  \myurl{http://www.literateprogramming.com/}{The Literate Programming
    web site}}

Donald E. Knuth has written the {\TeX}-system used to typeset this
report, and documented it by publishing the source code to the entire
system in four books.  The {\TeX}-system has impressed since it is of
an extremely high quality, both in code and documentation.
\textsf{Knuth offers money to those who find bugs in his code - on an
  exponential scheme}.

\textsf{references to the TeX book, MetaFont book, Literate
  Programming Book, nuweb (my observations)}

In order to write both documentation and code of such high quality,
Knuth developed his own method of coding \textsf{which he named
``Literate Programming''}, in which the author works with a single
file containing the documentation \textit{as it is to be presented to
the reader} written in a {\TeX}-dialect, with the actual code (with a
few characters escaped) listed in named chunks along with their
documentation, as it fits the author to present them.  This file
format is the \texttt{web}-format!

\begin{itemize}
\item 
The \texttt{weave} tool converts the web-file to a \texttt{tex}-file
to be processed by {\TeX}, and printed.

\item The \texttt{tangle} tool generates the actual code to be
compiled, by joining chunks with the same name, and inserting them in
other chunks that reference them (a simple macro expansion),
eventually producing one or more flat files which can be compiled.

\end{itemize}

\textsf{We definitively needs a sample of this!}

\textsf{Reference to literateprograming.org}

The problem with Knuth is that things that work well for him, does not
work quite as well for the rest of us.   No editors - not even Emacs -
can help where a given file is separated in a lot of regions, being 
either {\TeX}-code or source code, meaning that the editor cannot
provide the supporting functions which a programmer might have become
accustomed to.

You cannot code verbatim - some characters are reserved for other
purposes and must be written differently, which may be very annoying
to learn.  Additionally the concept of chunks all over the document
may be counter-productive if these are hard to navigate.

\textsf{What else}

\textsf{Many tried this- cloned the functionality -etc - I tried it,
  and wrote a few programs with it.}

Many people have experimented with the possibilities of a
weave/tangle pair resulting in several software packages, notably:

\myurl{http://www.ross.net/funnelweb/}{FunnelWeb},\myurl{http://www.eecs.harvard.edu/~nr/noweb/}{noweb}

\textit{http://w3.pppl.gov/~krommes/fweb.html\#SEC3}

There is
\myurl{http://www.webring.org/cgi-bin/webring?ring=litprog;list}{a
  webring for Literate Programming}, which is an excellent place to
look for further information.  The Collection of Computer Science
Bibliographies provides
\myurl{http://liinwww.ira.uka.de/searchbib/SE/litprog}{a search engine
  in literate programming publications}.

Oasis has \myurl{http://www.oasis-open.org/cover/xmlLitProg.html}{a
  page on Literate Programing with XML and SGML} \textsf{what about
  it? - note the references for working with the TEI DTD}

My previous experience with Literate Programs can be summarized as:

\begin{itemize}
\item The documentation gets bigger and better, simply because the
printed documents look better that way.  What would pass as a single
line comment in a source file, looks almost pathetic when typeset.
The full power of {\TeX} also encourages usages of illustrations and
graphs.  

\item The program development gets cumbersome.  You may have trouble
  using your favorite tools for editing, compiling and debugging.
  Users of most integrated development environments cannot use this
  model since the IDE does not provide hooks to provide this processing.
  
\item A critical point is whether the intermediate files are visible
  to the author! In order to be usable, \textit{all} derived files
  \textit{must} refer to the original document in a transparent
  fashion, meaning that the user should not have to worry about
  intermediate files.  (In the same way that the C-processor works
  under Unix).
\end{itemize}

My conclusion was that the Literate Programming paradigm does not pay
off as a individual programmer, but may work very well for a larger
programming team where good, current documentation is critical.

\subsection{Rational Rose - the other way around}
\label{sec:rational-rose}

\textsf{Talk to BNJ}.  Rational Rose - a UML development tool - uses
another approach, \textsf{work on meta-data with hidden code in the
  nodes, and generate code to actually run.  Allows
  reverse-engineering to make code development easier.
  Experiences from users}.  



\subsection{Compiling source code into an executable}
\label{sec:compiling-source-into-an-executable}

Most software projects does not consist of a single huge source file
which would take ages to compile, but of several smaller files, which
can be compiled individually and linked to an executable file.  If a
file with source code is changed, it is not necessary to recompile
each and every file but only those which depend on this particular
source file, and then relink the executable to incorporate the
changes.

This is possible because the authors has provided meta-data about the
system, regarding which source files the system contains, which
commands to call to compile and link the source files, which libraries
should be included, etc.  Normally an Integrated Development
Environment (like \myurl{http://www.borland.com/bcppbuilder/}{Borland
  C++Builder}, \myurl{http://msdn.microsoft.com/visualc/}{Microsoft
  Visual C++}, or \myurl{http://www-4.ibm.com/software/ad/}{the Visual
  Age products from IBM}) knows about these things, or a
\myurl{http://www.eng.hawaii.edu/Tutor/Make/}{Makefile} is constructed
to utilize the known relations built into \unixcommand{make} as well
as allow the author to add new ones.

In effect, adding meta-data allows the program development process to
go faster.

\subsection{\textsf{anything else with meta-data?}}


\section{Requiring multiple views of a document}
\label{sec:requiring-multiple-views-of-a-document}

\myimage{gr/zdnet-1}{The ZDNet presentation of a story}{zdnet-1}
\myimage{gr/zdnet-2}{The ``Printer-friendly version'' of
  figure~\vref{fig:zdnet-1}}{zdnet-2}

Some web-sites have so many advertisements and auxiliary links in
their layout, that they need an additional version which is
well-suited for printing (i.e. only has a single ad).  See
figure~\vref{fig:zdnet-1}
\myurl{http://www.zdnet.com/zdnn/stories/news/0,4586,2468874,00.html?chkpt=zdhpnews01}{for
  an example from ZDNet}, and figure~\vref{fig:zdnet-2} for the
``Printer-friendly version''.

\textsf{websites realizing that they cannot just offer an
  advertisement ladden service, without allowing for a reasonable
  paper version.  Since their HTML cannot ``remove'' the ads for
  printing, they have to offer two different version of each article.}

\myimage{gr/nokia-1}{Browsing the Internet on the large [sic] display
  of a Nokia 9110}{nokia-1}

These are two different \textit{views} of the same basic document.
ZDNet are expecting their visitors to either see their documents on a
CRT rendered by a reasonably capable browser or print them out on
paper.  That will probably change within a few years when it becomes
common for other devices than just traditional computers to be
connected to the Internet.  Mobile phones using WAP do not have as
much screen real estate (the Nokia 9110 is shown in
figure~\vref{fig:nokia-1}) so such users may with good reason prefer
short and crisp versions of the documents when they use a mobile
phone.  Nokia is working with 3com to produce a hybrid between a Palm
Pilot and a mobile phone.  Users have been known to request documents
in a format suitable for reading on a Palm Pilot.  

This tendency might as a side-effect be beneficial for the blind
computer users (see
\myurl{http://www.webring.org/cgi-bin/webring?ring=blind\&list}{the
  Blind Webring} for more information) .  They must normally use
text-to-braille software or a ``screen reader'' in order to use
computers, neither of which work well with web pages loaded with
graphics, but when web authors must write for many kinds of media
instead of just a particular version of a given browser, this will
also be beneficial to these users.  Of course, it is possible to write
ordinary HTML in a way that is usable by these users \textsf{statens
  guide for webpublikationer}, but that is rare these days.


\section{SGML/XML:  Generic formats for storing data and meta-data}
\label{sec:sgml-and-xml-generic-formats}

The problem of representing data in a generic way is not new at all,
and was solved for the printing industry in the 1970'ies by the design
of the Standard Generalized Markup Language (SGML) which is widely
used.  SGML was deemed too complicated for web browsers, and a trimmed
version was named XML and standardized in 1998.
Chapter~\vref{cha:sgml-xml-and-dtd's} talks a lot more about this.

For now it is sufficient to say that it is possible to store both data
and meta-data in a convenient form in a SGML or XML file, but that
these files must be rendered into the formats expected by the users,
like HTML, PDF or what else tomorrow may bring of new, exiting
formats.  Since SGML describes the \textit{content} and not the
layout, it is ``just'' a matter of creating a renderer for any new
format and add it to the collection.

This has been recognized by almost all of the ``Documentation
Projects'' (\myurl{http://www.linuxdoc.org/}{The Linux Documentation
  Project}, \myurl{http://www.freebsd.org/docproj/docproj.html}{The
  FreeBSD Documentation Project} and others) accompanying the various
free operating systems available on the Internet.


Chapter~\vref{cha:on-demand-rendering} talks about rendering documents
to a given format on demand (for example in a web browser).

\textsf{Others? It seems to me that stuff is missing here.} 


% \framepage{15cm}{
% Introduce publishing where HTML is just a backend amongst many (PDF,
% Word, ASCII).

% Describe why it is important to be able to render finished versions of
% documents fully automatically from a single \textsl{annotated} source
% (www/print/cdrom/Palm Pilot/braille/handicapped persons,whatever).  The better
% the annotation, the better the output (ref: Stibo).  Describe the need
% for SGML (history/usage) and XML (why/browser support/on-the-fly
% publishing/bleeding edge - being standardized).

% Donald Knuth - {\TeX}/Web/Literate Programming - advantages (high
% quality, excellent math, superb algorithms, can be tailored to needs
% (basic interpreter written in TeX) and
% disadvantages (programming language, not abstract) (designed 20 years ago).  Ask Steffen Enni
% about his thoughts.  Javadoc.  Perl POD.  DocBook projects (FreeBSD,
% LDP).  
% }

\section{The user should use familiar tools to publish documents}
\label{sec:the-user-should-use-familiar-tools-to-publish-documents}

\framepage{15cm}{
SGML-editing is hard and tedious - this is one thing that the folks in
the \myurl{news:comp.text.sgml}{\texttt{comp.text.sgml}} and
\myurl{news:comp.text.xml}{\texttt{comp.text.xml}} newsgroups agree
upon.  Tools are essential for authors.

Very few tools exist, and the good ones are quite expensive.  The
general consensus is that the best OpenSource tool is the Emacs editor
with the PSGML package (see~\vref{sec:emacs-with-psgml}), and that
this tool is primarily suited for programmers and other people who are
well acquainted with SGML and XML.

\textsf{PRINT DIRECTLY TO A PDF FILE - WEB PUBLISHING}

Computers should \textit{help} you do your work, and users are
generally most productive with the tools they know.  Why should an
author use SGML with an editor she loathes, if precisely the same result can be
achieved by using a word processor she is familiar with?

In order to automate the conversion from e.g. Word to the equivalent
SGML file, it is important that the author uses only those
constructions that the conversion program understands.  This is most
likely given as a set of macros that must be used, as well as a
verification program which the author can use at any time to check
whether her document is conformant.

Publishing a document to the web should be as easy as printing a
document.  The basic principles are the same - you can do with a
``Publish''-button and a dialogue box where the essential information
is provided.  It is not so today - discuss reasons .  Use ``print to
fax'' software as horrible example of this idea gone wrong.

When the webserver is dumb, you cannot do much.  If there is a
database underneath, much more is possible.  Discuss the idea of
having several ways of entering documents in the database (upload via
form, send as email (fax software can do this too), print to virtual
printer, scan, fax, voice) and letting the software do the
conversions.

Use example with print PostScript to file and upload via ftp to
printer (LexMark).

Writing XML directly is much harder to do than writing HTML (stricter
syntax, more options, plainly just more to type), and should be aided
by a good tool.  Alternatively, the user should use well-known tools
(Word) and mark up according to very strict rules, which is then
automatically converted to the XML document.  This does not give as
rich documents, but allows users to publish existing documents with
very little trouble.
}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "rapport"
%%% End: 
