% $Id$

\chapter{SGML, XML and DTD's}
\label{cha:sgml-xml-and-dtd's}

\mycitation{The greatest improvement in the productive powers of
  labor, and the greater part of the skill, dexterity, and judgment
  with which it is anywhere directed, or applied, seem to have been
  the effects of the division of labor [between different
  craftsmen].}{Adam
  Smith}{\myurl{http://www.wsu.edu:8000/~dee/ENLIGHT/WEALTH1.HTM}{An
    Inquiry into the Nature and Causes of the Wealth of Nations}
  (1776)}

% \mycitation{If you don't read the manual before you enter the
%     dungeon, you might end up being killed}{Rumour in the Nethack
%   computer game}{about 1993}

A major ingredient in developing large, complex systems with several
layers of data and meta-data, is the ability to \textit{abstract}
between different levels of the system.

\begin{itemize}
\item
  
  \textit{Operating systems} provide the ``file'' abstraction, so the
  programmer does not have to think about the underlying hardware when
  reading and writing information, and these operations are normally
  optimized for speed.

\item
  
  Expensive printers provide the \textit{PostScript language} which is
  an abstraction for ``electronic paper''.  Any Postscript file can be
  printed to any PostScript printer, without considering the specifics
  of the device.  This was a major step forward from the traditional
  matrix printer where the application had to generate bitmaps for the
  printer to print.
  
\item
  
  \textit{Programming languages}, like C, provides the abstraction of
  a ``virtual machine''.  The programmer does not have to worry about
  the physical size of a character, and what the best way to implement
  a loop on this particular processor is.  The compiler takes care of
  that, leaving the programmer to concentrate on problems at a higher
  abstraction level.
  
\end{itemize}



The same need arose in the printing industry, where SGML (Standard
General Markup Language) was developed in
\myurl{http://www.sgmlsource.com/history/roots.htm}{the late sixties
and early seventies} to facilitate a transfer from ``specific coding''
to ``generic coding'' (Goldfarb mentions using ``heading'' instead of
``format-17'').  This allows authors to concentrate on \textit{what}
they write, instead of \textit{how} it is presented.  In Goldfarb's
own words in 1971:

\begin{quote}
  The principle of separating document description from application
  function makes it possible to describe the attributes common to all
  documents of the same type. ... [The] availability of such 'type
  descriptions' could add new function to the text processing system.
  Programs could supply markup for an incomplete document, or
  interactively prompt a user in the entry of a document by displaying
  the markup. A generalized markup language then, would permit full
  information about a document to be preserved, regardless of the way
  the document is used or represented.
\end{quote}

Today, almost 30 years later, this is coming to the users of the world
wide web, as the presenting technology moves into the web browsers -
with XML being defined as a subset of SGML - and they will be able to
fully present XML documents individually tailored to the users
preferences.  The author can provide richly annotated content, along
with a suggested way to view the content (in form of a style sheet).
The user may freely choose to override some or all of these suggestions.

This is a radical change from the current situation where HTML allows
an information provider to be so specific in markup that it makes it
unusable in some browsers, and without any means for a user to specify
otherwise.    An example is the font size change commands, which can
render a site totally unreadable in Netscape.

Hopefully the mechanisms for user customizations will be so well
developed when XML is commonplace in browsers and web sites, that 
users can disable this behaviour.

It appears to me that there will be two major uses for XML, one is for
transporting data between applications (like a database and a
browser), and the other will be as a media for interchanging
information between humans.  My expectations are that a few, very well
designed, standard XML definitions will be created and that most
public XML documents will be written to comply with one or more of
these standards, and web robots (like search engines and web crawlers)
will know these standards and be able to extract meta-data from them.

The initial XML-editors are already arriving.  It will be very
exciting to see what they can do in 5 years.


% Looking from the DocBook DTD's this \textsf{section} describes XML,
% how it was developed from SGML inspired by the short comings of HTML,
% and how it is used,` including viewing, authoring, and conversions
% (transforms, formatting).

\section{``HTML is a SGML DTD'' - the concepts}

\myvref{tab:programming-langugages-and-html} shows how the most
frequently used acronyms relate when mapped to the programming domain.
See \myvref{cha:terms-and-concepts} for the full descriptions.


\begin{table}[htbp]
  \begin{center}
    \begin{tabular}{|l|l|}
      \hline\hline
      \textbf{Documents} &  \textbf{Programs} \\
      \hline
      SGML &Family of programming \\
           & languages  (C/Pascal/Lisp...) \\
      \hline
      XML  & Another family of programming \\
           & languages. (XML is a subset of SGML) \\
      \hline
      DTD  & Backus-Naur notation of a \\
           & given language, defining how it looks\\
      \hline
      HTML & A certain programming language\\
           & like C \\
      \hline
      XHMTL & Another programing language like Java \\
           & Well written programs can run both in C and Java\\
      \hline
      DSSSL & A report generator which takes a SGML program \\
           & as input and produce a report, another\\
           & program or something else \\
      \hline
      XSL  & A report generator which takes a XML program \\
           & as input and produce a report, another\\
           & program or something else \\
      \hline
    \end{tabular}
    \caption{The analogy between programming languages and SGML}
    \label{tab:programming-langugages-and-html}
  \end{center}
\end{table}

Basically SGML (Standard General Markup Language) covers a large
family of document markup languages, which all share that a given
SGML-document \textit{must} comply to a DTD (Document Type
Definition).  DSSSL is used to convert a SGML document into something
else.  DSSSL's are written in a Scheme like language (see
\cite{Dybvig:1996:SPL} for a good reference manual).

XML is a light-weight version of SGML designed to be embeddable in a
browser.  XML-documents \textit{may} comply to a DTD, or be
stand-alone (``DTD-less'').  \textit{XSL} is used to convert a XML document
into another XML document with the \textit{XSLT} process.  The XML-FO
(Formatting Objects) dialect is specially designed for printing the
document, since that is intended to be a generic representation of the
printed version (much similar to a PDF-file).

XHTML is a XML-version designed to be presentable by HTML-4.0
browsers.

\section{The author should provide content, not do layout}

SGML came originally to because the authors at IBM were not
consistently marking their documents up, resulting in suboptimal
documents.  This was because they were doing physical markup instead
of logical markup.

%\mydetour{
  Modern word processors have for a long time been constructed around the
  ``What you see is what you get'' paradigm, where a great deal of
  effort has been spent on allowing the user to see exactly on screen
  what would be printed on paper.  Unfortunately the limited
  resolution and size of a modern computer monitor cannot show the
  whole page in high resolution, and the user must therefore work in
  small ``regions'' without being able to see the whole page, or
  perhaps even several pages. Microsoft Word provides a draft mode
  where just the text is shown without markup; WordPerfect provides a
  ``Reveal Codes'' mode where the user can edit the text along with
  the tags.
  %}
  
Recent versions of Word and WordPerfect have had styles built in,
where the user could specify ``heading'' and similar categories for
selected text, but have been very passive in helping users to employ
these categories, meaning that most of these document have had almost
no semantic markup, but only visual markup.  [Note: WordPerfect 2000
is reportedly capable of editing SGML documents directly given the
appropriate DTD.  This has not been verified]

In combination with this the web publishing tools by Microsoft and
others have blatantly ignored web standards, and used all kinds of
font modifying tags instead of the HTML-tags conveying \textit{meaning}
about the text.  These tags often implement the particular settings
that the author had on her word processor, so that the author is
effectively doing layout on the web.

This is not a new phenomenon, however.  The best way to typeset
mathematics have for more than a decade been the
\myurl{http://www.tug.org}{\protect{\LaTeX} system} where the author basically is
told to write what she wants to and let {\TeX} do the rendering.  Even
so, it is often very tempting even for the most dedicated purist to
want to change the layout to make a small visual change, simply
because they \textit{can}.  Unfortunately very few have the expertise
to do this right, so the changes usually make it worse.  I have seen a
book where the author had prepared camera-ready copy for his
{\LaTeX}-document.  The book was printed in A5, but he had apparently
handled this merely by increasing the margins of a normal A4 page, and
nothing else.  The result was that the layout looked awkward - the
line spacing seemed too large for the page.  This would not have
happened if a professional layouter had produced the book!

A layouter may also want to change the whole layout of a book, for
example if a new edition is to be part of a series with another
distinct style.  The publisher may want to turn the book into webpages
or a CD-ROM, or into a Braille version for blind persons.  It might
even be read aloud by a computer.

%\mydetour{
  The publisher ``\myurl{http://www.ora.com}{O'Reilly and
associates}'' is well-renowned for providing high quality reference
books, and have for quite some time had their authors write in SGML.
A very visible benefit of this, is that creating indexed HTML-editions
of their books on CD-ROM is very easy since it basically is just a
matter of applying a different style sheet to the documents in
question.  Recently they have made
\myurl{http://www.oreilly.com/people/staff/crism/dsssl/orastyle/}{their
DSSSL scripts for the purpose available on the net}, allowing others
to do the same.
%}

It should not bother the author during the writing process how the
finished document will look, as long as she marks up the document
appropriately, providing suffiecent meta-data about the content.  A
good and fast preview function will, however, be very beneficial in
order to allow the author to correct errors immediately and create
visually pleasing prints for proof-reading.

As the possible meta-data annotations are decided by the DTD, it is
important to choose a DTD carefully.  A well annotated document can be
used for many purposes, not all of which was envisioned when the
document was written.  Ten years ago no one had heard about the world
wide web - within the next few years, it will probably be the main use
of SGML in the world.


% \textsf{!!} The \textsf{needs of blind people} are very different
% from the \textsf{needs of mobile phone users}, yet 

% \subsection{The design and evolution of SGML}

% \framepage{15cm}{{Look up SGML history when/where/whom/\emph{WHY}}.
%   What does SGML do well?  Why is it so?  Initially hard to write in.
%   Development of DTD's to use in applications, with Stylesheets (FOSI,
%   DSSSL (when)).  Development of tools for making writing easier
%   (complex, expensive, slow).  Use liking to printer drivers for
%   printers, which was sweeped away by PostScript for LaserPrinters
%   (bitmaps are too hard to handle - talk about Stibo rendering in 2400
%   dpi to guarantee idential prints).}

% \section{The \textsf{creation} of HTML}

% \textsf{Look up creation of HTML on the net.  Look for reasons why HTML
%   was chosen to be a SGML DTD instead of e.g. Windows Help, or
%   something else}.

% \myurl{http://www.w3.org/History/19921103-hypertext/hypertext/WWW/MarkUp/MarkUp.html}{The
%   w3c have notes and DTD's describing the first versions of HTML}.



% \framepage{15cm}{
%   HTML: 1.0 (original, simple, $<$hr$>$ tag was added
%   due to popular request), 2.0, 3.0 never made it, 3.2 tables, 4.0 w3c
%   straightens up things.  Netscape added new tags \emph{ad hoc} often.
%   In the mean time HTML has been made to do things it was never meant
%   to do originally, for presentation purposes (large imagemaps in
%   tables without any textual information), and there is still a need
%   for new features that designers want.  \emph{W3C saw that blindly
%     adding new features to HTML would result in an even more bloated
%     standard} with a lot of backward compatability to maintain - a
%   modern browser must still be compatible with the bugs in Netscape
%   versions 2 and 3.  These were renowned for being very lenient
%   towards users providing erroneous HTML, and have started a tradition
%   of browsers doing their best to interpret what the users
%   \emph{might} have meant.  This makes it hard to use HTML as a
%   generic information data format, since parsers are complex due to
%   the many exceptions.  }

\section{Why the need for XML when HTML is available?}

The basic idea behind HTML has proven sound, namely to have a document
format which is so simple that it is easy to write by hand in an
editor and have computers generate automatically. The problem is that
the current features of HTML are based on whatever the engineers at
Netscape and Microsoft decided would be nice in the newest version of
their respective browsers.  The submittal for standards approval
happened afterwards in order to have the HTML-implementations being
defined as the reference (as happened with HTML-4.0).

Since the original version of HTML it has been made to do things it
was never meant to do originally, like presentation purposes (large
imagemaps in tables without any textual information), and there is
still a need for new features that designers want.

The W3C saw that blindly adding new features to new
versions of HTML would result in an even more bloated standard with a
lot of backward compatibility to maintain - a modern browser must
still be compatible with the bugs in Netscape versions 2 and 3.
Additionally HTML-browsers have a tradition of being very lenient
regarding acceptance of incorrect HTML (with a lot of guessing of the
users intent).  There was not much that indicated that HTML could
become a simple, well-defined and consistent standard, so the W3C took
a bold step in saying: ``We need a new format - the eXtensible Markup
Language.  XML!''

XML was as HTML built on SGML, but with a much stricter syntax than
HTML and a lot less facilities than in SGML.  XML is a new standard
even by Internet standards - it was made a W3C recommandation in 1998,
and the accompanying XSLT and XPath were made W3C recommendations in
November 1999.  

% \textsf{What was the design goals}

The resulting language for designing languages has these features:

\begin{itemize}

\item A very strict syntax which every XML-document must comply to
(which is what a \emph{well-formed} document does).  All open tags
must be closed explicitly.  Tag attribute values must be in quotes.
  
\item Easy to parse and generate.  The strict syntax makes the parser
much simpler than a SGML parser, and the strictness make it smaller
than a HTML parser.

\item The DTD is not mandatory.  DTD-less documents does not have a
  DTD to conform to, and must only be well-formed.  This allows the
  use of XML as a dynamic data description language where the input is
  unknown beforehand.  The DTD can be applied when the need arise to
  validate the input.  A well written DTD can be used both for the
  SGML and XML versions of a given document, without change.

\item
  XML is based on Unicode, allowing for easy use of non-ASCII
  character sets.  This is most likely also a reason why most
  implementations have been written in Java so far, as it is one of
  the few languages which support Unicode directly.
  
\item The XSLT process is designed to map a style sheet onto a given
  XML document, producing another XML document.  XSL-FO is a special
  case for producing renderings of the XML document designated for
  visual inspection.  The XML-FO files requires a final processing
  step to be rendered to the final representation.
\end{itemize}

This approach has so far been a very big success.  There is a large
number of XML-parsers freely available for a lot of languages, which
makes it very easy to allow a given program to read in data from an
XML-file.   The XML-format will most likely replace most of the simple
line based file formats, like configuration files in both
DOS/Windows/NT and Unix, as well as take over the role of an
intermediate data format between two programs (much like the comma
separated data file used in the database world).

\section{XHTML - XML compliant HTML}

A big problem with XML in general is that it cannot be presented by
anything the majority of users have today.  It is possible, however,
to specify a form of XML called
\myurl{http://www.w3.org/TR/xhtml1/}{XHTML} which bridges the gap
between the two worlds of XML and HTML.  Documents corresponding to a
XHTML DTD can be presented by HTML-4.0 compliant browsers.  The
differences from HTML-4.0 are:

\begin{itemize}
\item Elements (the HTML tags) must be closed correctly.
  
\item Attribute values must be quoted.
\item Element names and attributes must be given in lower case.
  
\item Empty elements must either have an end tag, or the start tag
must end with ``/$>$''.  I.e. a horizontal line which is written as
\tag{hr} in HTML, must be written as ``$<$hr /$>$'', where the space
is important to make this acceptable to HTML-4.0 parsers.
%\item \textsf{More stuff... Netvwork lag}
\end{itemize}

The \myurl{http://www.w3c.org}{Tidy} tool from W3C can be used to
convert a HTML-file to XHTML automatically.  A typical example of the
changes necessary in the markup is shown below.

\begin{verbatim}
HTML: <hr noshade>
XML:  <hr noshade="" />
\end{verbatim}

This is a very important XML-dialect since it allows the generation of
viewable web pages with XML-tools only, and allow such a XHTML file to
be \textit{read} for further processing by XML-tools.

This is conceptually very different from the usual situation, namely
that HTML is the final result from \textit{formatting} a document, as
opposed to just another step in XML-transformations.


% \textsf{Note: XHTML style sheet generated files does not trigger
%   correct change to UTF-7 characterset.}

% \textsf{WHICH CONVERSION UTILITIES ARE THERE?  TIDY?  READ XHTML?
%   WRITE XHTML?  WHAT DO W3C SUGGEST?  WHY DO THEY THINK THIS IS A GOOD
%   IDEA}

\section{Which dialect of SGML should be used?}

The Document Type Definition (\emph{DTD}) defines exactly the way a
SGML/XML document can look, when conforming to the DTD.  Some of the
things dealt with, are:

\begin{itemize}
\item Which tags are valid
\item Which tags can appear at any given part of a document
\item Which attributes are valid for a given tag
\item Which \emph{entities} (macros) can appear in a document, both
for expansion strings but also for Unicode characters unavailable to
the document author
\end{itemize}

There are a lot of different DTD's available for many different
purposes, since basically everywhere a datafile is needed XML can be
used with a suitable DTD.  Examples are ChemML for describing chemical
content, and SVG for describing vector graphics. If a need arises it
is just a matter of creating a suitable DTD to work with it.

Unfortunately, for each and every pair of DTD and output format an
XSL-style sheet must be written, tested and maintained.  Therefore it
is a very good idea to use a commonly used, documented, and
well-thoughtout DTD for the documents, and several other groups have
already done such work. 


% The Cactus system uses the DocBook XML V3.1.7 DTD with the
% DocBook XSL \textsf{-------------------}

\section{How can a *ML document be viewed?}

Users do very rarely want to view XML-documents directly.  In order to
render an XML document into something viewable, a \textit{style sheet}
must be applied to the document (unless it already is an XHTML document).
Style sheets convert the tags in the document to a given output
format, where the W3C already has defined XML and XSL-FO as targets.

%\textsf{GRAPH DEPICTING STYLE SHEETS BEING APPLIED FOR DIFFERENT
%  OUTPUT FORMATS}

Commercial products usually have a viewer which can apply some style
sheets directly%
%\texttt{examples? Panorama? Framemaker+SGML}
, but are due to licensing costs and supported user base rarely an
option for general for documents targeting end-users, and I have not
tried any of them.

% \myvref{tab:distribution-formats} discusses the
% current \textit{de facto} document formats on the Internet.


% \begin{table}[htbp]
%   \begin{center}
%     \begin{tabular}{|l|p{10cm}|}
%       \hline\hline
%       Format & \\
%       \hline

%       HTML & Browsers are available for any modern platform.  HTML-4.0
%       compliant browsers include Netscape Navigator 4 and Internet Explorer
%       4 \\ 
      
%       PDF & Adobe Acrobat Reader is available for most mainstream
%       platforms from \myurl{Adobe}{http://www.adobe.com}.  The Adobe
%       Acrobat Viewer is available for any platform with Java.  The
%       Open Source project
%       \myurl{http://www.ghostscript.com}{Ghostscript} is available for
%       very many platforms. \\
      
%       Microsoft Word & The \texttt{doc} and \texttt{rtf} file formats
%       are widely used, but needs a full word processor to format
%       properly.  Anything else but Microsoft Word gives inferior
%       results, and Word only runs on Windows.
%       \\
%       \texttt{tex} and \texttt{dvi}& These require some part of the
%       {\TeX} system to be installed on the recipients computer.  This
%       is rare for non-academic people.\\
%       PostScript & This is perhaps the most generic representation of
%       a printed document and would fit well for most purposes.
%       Unfortunately only few people have a PostScript interpreter
%       installed on their computer\\
%       \hline
%     \end{tabular}
%     \caption{Frequently encountered document distribution formats}
%     \label{tab:distribution-formats}
%   \end{center}
% \end{table}

The most portable way for web users is to convert the XML to HTML
server side, and then provide for an alternative rendering into PDF if
the user needs to print the document.  The available Open Source
software for this is surprisingly small, but that is rapidly changing.
Please note that XML style sheets still implies SGML style sheets,
which is interesting because these are more mature as they have been
used for a longer time.  The browsers are not yet capable of doing
these transformation themselves, but upcoming versions show promise:

The Microsoft Internet Explorer will most likely be the first full
browser to have XSLT built in.  Version 5.0 can display an XML
document as a tree of tags which can be navigated, but without full
XSL capabilities.

The Microsoft Internet Explorer 5.5 (not yet available at the time of
writing) will be able to transform XML documents with XSL-style sheets
internally, complying -- hopefully -- with the W3C recommendation.
The previous version of IE was available on Windows, Solaris and HPUX.

The Mozilla browser in its pending release as Netscape Navigator 6.0
is as of the Milestone 15 build not capable of applying XSLT to XML
documents, but there are forces working for building it in in order to
have full XML with XSLT support in the released version of Netscape
6.0.

For the immediate future web publishing in XML should be done server
side, with an XSLT rendering engine like Cocoon.


% \subsection{SGML style sheets}
%
% \textsf{CONFIRM THE BEHAVIOUR WRITTEN BELOW IS EXCACT}
%
% SGML style sheets are \textsf{written in DSSSL} (others?), which is
% a Lisp-dialect, and the usual Lisp-conventions apply.  Several
%
% \textsf{several?} \textsf{what can this do?}
%
% The DocBook reference recommends using \texttt{jade} written by James
% Clark, to do DSSSL conversions.  \textsf{long processing times later
%   implemented in XT with threads to allow faster output}.  
%
% \textsf{Show a sample stylesheet}

\section{Cocoon and XSQL - tying XML, SQL and XSLT together }

\mycitation{Once you've understood what the XSQL Page Processor
  running inside the
  XSQL Servlet does, you'll realize that Oracle XSQL Pages is really
  just a very thin layer of convenience functionality allowing you to
  leverage the tremendous flexibility and power of SQL, XML, and XSLT
  to
  really do the ``heavy lifting''.}{Steve Muench}{\myurl{http://technet.oracle.com//tech/xml/xsql_servlet/}{http://technet.oracle.com//tech/xml/xsql_servlet/}}

XSL - the XML style sheets - are written in an XML-dialect, where the
tags in the name space ``xsl:'' describe what is to be done with the
source XML tree, in terms of tag-remapping (\tag{para} to \tag{p}),
sorting of subtrees, and several programming constructions like
``if'', ``while'' and ``foreach''.  New tags can be constructed,
existing tags can be altered, and it should be sufficient for most
tasks.  Templates are used to specify \textit{when} and \textit{where}
a given action should be taken.   XSLT refers to the \textit{process}
of applying an XSL to a XML tree.


% {\small
% \verbatiminput{x-outline.xsl}
% }

\myimage{gr/outline-xsl-1}{Outline of the sample DocBook document in \myvref{sec:amanda-readme.xml}}{outline-xsl-1}


The style sheet listed in \myvref{fig:listing-x-outline.xsl} -- taken from
the DocBook distribution -- creates an outline of a DocBook document
by looking for headline tags, and make a tree of them. Line breaks
have been introduced for readability.  See
\myvref{fig:outline-xsl-1} for a sample run.

\begin{figure}[tbp]
  \begin{center}
\small
\listinginput{1}{x-outline.xsl}    
    \caption{The x-outline.xsl style sheet to generate an outline of a document}
    \label{fig:listing-x-outline.xsl}
  \end{center}
\end{figure}

As discussed previously the XSLT process is currently only viable to
use on the web server, and just the resulting HTML or XHTML-pages
should be sent to the browser.  Few OpenSource projects at the time
strive to implement such a facility in web servers, because the
standards are so new.

One of the most exciting things about this new technology, is that you
may have specialized processors in an XML-data pathway.  The
XSLT-processors have already been mentioned, but others are beginning
to appear.

I decided to evaluate the
\myurl{http://xml.apache.org/cocoon/index.html}{Cocoon project} which
is an Apache project, but has it as their goal to be usable in any
servlet-capable web server.  To the hosting webserver Cocoon looks
like a set of servlets -- to the XML-writer Cocoon provides at least
the following

\begin{description}
\item[XSLT] -- Documents can be transformed by a given XSL-document.
  
\item[SQLProcessor] -- A JDBC-connection to an SQL database can be
  established, a statement executed, and the result returned as a
  user-specificed XML-tree, suitable for further processing.
  
\item[XSQL] -- A very similar project to SQLProcessor from Oracle,
  which is geared towards dealing with Oracle databases, both with
  servlet-technology but also as stand-alone, where Oracle have
  provided tools to enter XML-tables (preferrably in the format of the
  output from another XSQL-query) into a similar Oracle database, thus
  allowing Oracle to be the end-point of a series of XML-operations.
  This is much better than generating SQL-statements for each item
  individually, since the technicalities with optimizing batch-job
  submittal, doing \texttt{commit} regularily, etc. are dealt with
  automatically.  The process is purely data driven.   XSQL was
  released March 2000.

\item Perform transformations based on the browsers identification
  string.
\end{description}

A site-wide layout is easy to implement, just be applying a standard
style-sheet on the root element, for each and every document, and
ensure that all style-sheets used include the global site
configuration sheet.  I did this for the Cactus presenters and it is
very easy to maintain.

My general feeling with working with the XSLT technology (while
writing presenters for Cactus in Cocoon) have been that the basic
approach of keeping the information to go before, in and after any
given template is a very efficient way to do this (all the relevant
information is placed in a single place), and requires a line of
thought similar to programming in a functional language.  See lines
14--20 in \myvref{fig:listing-x-outline.xsl} where the \textit{root
  tag} (named ``/'') is replaced with a \tag{html} and a \tag{body}
tag which then contains the templates contained within the root tag.

Since the DocBook reference is very vague on style sheets (the book
went to press before the XSLT recommendation was finalized) the
Internet has been a great help.
% \myvref{tab:leading-xslt-processors}
% lists the tested XSLT processors which conforms to the W3C
% XSLT-19991102 recommendation.

% \textsf{Where should I talk about SAX?}

% \begin{table}[htbp]
%   \begin{center}
%     \begin{tabular}{|l|p{10cm}|}

%       \hline\hline
      
%       XT &  XSLT processor based on the XP parser.  Both
%       written in Java\textsf{ by James Clark}. \\

%       Xalan & XSLT processor written in Java originally based on the
%       LotusXSL processor by \textsf{alphaworks?}.   Currenly under
%       heavy development by the Cocoon team 
%       (\texttt{xml.apache.org}). \\
      
%       \hline
%     \end{tabular}
%     \caption{Leading XSLT processors implementing
%       \textsf{XSLT-19991102} in February 2000} 
%     \label{tab:leading-xslt-processors}
%   \end{center}
% \end{table}

When choosing a XSLT-processor it is either C-based or a Java-based.
A frequent concern about Java programs is whether it is fast enough.
During the initial evaluation of Cocoon it was found that Java tools
are quick ``enough'' for casual use, even though they are not yet
tuned for optimal performance.  The Oracle XML parser, and XSTL tools
(as of May 2000) are available in both a C++ and a Java version.
Since the operations are purely datadriven it should be irrelevant
which language the conversion tools were written in.

Even so, implementations in C should only be considered by designers
if the Java solution is clearly too inefficient for the job in
question.  Some of the advantages of Java over C in program
development are:

\begin{itemize}
\item Very hard to do pointer access.
\item A large set of tested libraries.
\item ``Write once, run anywhere'' -- no recompilation is needed when
  moving to another platform, and a simple distribution file format.
\end{itemize}

\begin{figure}[tbp]
  \begin{center}
    \listinginput{1}{../src/public_html/incoming.xml}
    \caption{The \texttt{incoming.xml} file which is to be rendered by
      \texttt{incoming.xsl} in
      \myvref{fig:src-public_html-incoming.xsl}} 
    \label{fig:src-public_html-incoming.xml}
  \end{center}
\end{figure}

I started using \myurl{http://xml.apache.org/xalan}{Xalan} with Cocoon
which at that time was in a development state, and contained bugs
which causes it to crash on some of my sample DocBook XML document.
After that I turned to XT by \myurl{http://www.jclark.com}{James
  Clark} which is has proven to be stable and reliable, and that I
have used for the rest of the project - my only complaint is that it
stops after reporting the first error instead of parsing the whole
document.


When the Cocoon project stabilizes they will have a
high end XSLT-engine embedded as a servlet in Apache, which makes this
a project to watch.  The technology is at the time of this writing
still under heavy development, but very promising.  I have used Cocoon
to create \textit{presenters} in the Cactus sample implementation.
%described in \myvref{sec:cactus-implementation}. 

A sample set of XML and XSL files\footnote{These files select and
  render all entries from the \texttt{incoming}-table.  Since that
  table contained about a thousand entries, it was a good test of the
  speed of the processing speed of Cocoon.  It is very slow.  Even
  considering it needs to read the whole result in before processing
  it, it is slow.  This technology is currently suitable for small
  files only} are listed in \myvref{fig:src-public_html-incoming.xml}
  and \myvref{fig:src-public_html-incoming.xsl}.  The
  \tag{?cocoon-process type="sql"?} PI (Processing Instruction) causes
  the \tag{connectiondefs} and \tag{query} tags in the XML-file to be
  processed by the SQLProcessor.  The \tag{query}-tag in lines 18--21
  uses the JDBC-connection defined in lines 9--16 to execute a
  \texttt{select ... from incoming}, and insert the result in place of
  the \tag{query}-tag, with the column-names as tag-names.

Note the \texttt{count(*) as l} part -- this is necessary as the
SQLProcessor doesn't validate the column names before trying to insert
them as tag-names in the XML-tree.  Parenthesis are not allowed as
part of tag-names, so using an ``as''-term allows you to give it an
acceptable name.

The XSLT-process (enabled by the \tag{?cocoon-process type="xstl"?
  }-tag) then applies the style sheet in
\myvref{fig:src-public_html-incoming.xsl} to the result tree.  Lines
17--22 applies to each \tag{ROWSET}-tag, which is the default tag
around the whole result returned by SQLProcessor.  This is replaced
by a table with a border and a background, where the first line is
pre-generated to be a header line naming the columns.

Lines 25--35 matches each ROW in the result (which is the default
SQLProcessor tag for a row in the result).  A table row is generated,
where each field is explicitly generated with a \tag{td}-tag with a
content value generated with \tag{xsl:value-of select="name"}, where
\texttt{name} is the content of a tag inside the current ROW-tag.

Such a result from SQLProcessor might look like:
\begin{verbatim}
<ROWSET>
  <ROW>
    <when>1999-11-25 12:15</when>
    <user>ravn@mip.sdu.dk</user>
    ...
  </ROW>
  <ROW>
    <when>1999-11-25 12:20</when>
    <user>ravn@mip.sdu.dk</user>
    ...
  </ROW>
</ROWSET>
\end{verbatim}

By putting the \tag{connectiondefs} tags into a separate file, using
the \texttt{<!DOCTYPE page [ <!ENTITY connection_defs SYSTEM
"sql-defs.xml"> ]> } invocation in the header of the XML file to
define the entity with an external content, and using
\texttt{\&connection_defs;} to actually include the file, it is
possible to have a single file with the definition of the
JDBC-parameters, allowing for easy maintenance.

% The platform independance of Java indirectly prompted the
% implementation of remote filters in Cactus.  Very early in the
% evaluation Xalan threw the above mentioned exception when processing a
% medium sized DocBook document.
% In order to rule out errors in the
% underlying Java environment on Asserballe (see
% section~\myvref{sec:asserballe} for technical details) an identical run
% was made with JDK 1.2 on Aalborg.  The same error occured, but in less
% than one second instead of ten.  Experiments showed that Java programs
% consistently runs 15 times faster on Aalborg than< on Asserballe.
% \textsf{and so what?}
\begin{figure}[tbp]
  \begin{center}
\listinginput{1}{../src/public_html/incoming.xsl}
    \caption{The \texttt{incoming.xsl} to be applied to
      \texttt{incoming.xml} in
      \myvref{fig:src-public_html-incoming.xml}} 
    \label{fig:src-public_html-incoming.xsl}
  \end{center}
\end{figure}

\section{Converting documents to XML}

XML is currently having the same problems that HTML had in its initial
years, namely lack of software support.  The difference is that
programs rendering HTML are so forgiving that it is easy to write HTML
by hand in a text editor.  It is not so with XML, as it is very strict
and unforgiving.  The XML standard allows any parser to abort
executing at the first error occurring.

The circumstances are different now, however.  The web browsers have
clarified the enormous need for programs capable of creating proper
HTML and XML, and software developers world wide are recognizing XML
as being a good standard way of representing structured data based on
Unicode.

Even so there is still a severe lack of conversion tools for the many
files out there in non-SGML based formats like Word, Excel, {\LaTeX},
{\TeX}, Lotus Notes, etc., which must be available to the public
before the documents can be converted.   I have found very few
cross-platform software products which can do this:

\begin{table}[htbp]
  \begin{center}
    \begin{tabular}{|l|p{10cm}|}
      \hline\hline
      \myurl{http://www.tetrasix.fr}{Majix} & Java based RTF to XML
      converter on any Java platform.  Can invoke Word automatically
      to convert to RTF.\\
      pod2docbook & Converts Perl documentation to DocBook SGML.  I
      have patched this to optionally create XML instead.\\
      wv & Converts Word 6-95-97 to HTML.  Since wv
      knows what kind of object it is currently parsing, it would be a
      good project to adapt the source to generated DocBook XML
      instead. \\
      tex4h & Package for {\LaTeX} which can extract the content from
      a given \texttt{tex}-file, while accounting for the physical
      markup and any macros and similar.   tex4h may require up to
      seven passes of the original source file and derived files in
      order to get the full meaning of complex tables. \\
      \hline
    \end{tabular}
    \caption{Conversion tools to XML [February 2000]}
    \label{tab:conversion-tools-to-xml}
  \end{center}
\end{table}

\subsection{Majix - RTF/(DOC) to XML}

Majix is an RTF (for Word 97
documents) to XML converter.  I have created a configuration and a
small style-sheet which can render a RTF file to DocBook XML.

If run with the Microsoft Java Machine (\ntcommand{jview}) while
logged into a Windows machine with access to the GUI, it can launch
Word to convert a given DOC file to RTF and process it, with very
reasonable results.  I have experimented with making this accessible
to Linux by installing Microsoft Services for Unix on a NT machine
with Word installed, and using telnet to invoke Majix.  Unfortunately
this doesn't work, because Word hangs indefinitely apparently trying
to contact the GUI.  A possible way to circumvent this could be having
a pseudo-user watching a directory on a NT-server and launching Majix
whenever DOC or RTF documents arrive.  These can at best be described
as work-arounds.

Even though it shows great promise, it has been a year without new
releases (including the promised Pro version), and the source is not
available.  

Conclusion: The Majix software is currently best for personal end-user
conversions.


\subsection{pod2docbook - POD to XML}

As described in \myvref{sec:perlpod} the POD format was designed for
writing documentation for the Perl programming language with the
intention of being ``\ldots an idiot-proof common source for nroff, TeX,
and other markup languages, as used for online documentation'' by
Larry Wall.
       
The original release produced SGML DocBook.  I have submitted patches
to the author which allows the user to choose between XML and SGML
DocBook.

Conclusion:  This do a good job for documents written in POD, and will
probably with time take over as the default formatter for printed
versions of Perl documentation.

\subsection{tex4h - convert {\TeX} to XML}

The standard answer in the \texttt{comp.text.tex} newsgroup for people
asking how to convert \texttt{tex}-documents to XML is ``Use tex4h''.
I have not had the time to evaluate it, but have seen from several
sources that this program truly jumps though hoops to extract the
content from the original source file, while at the same time allowing
the author to use a large range of the commonly used {\TeX} commands,
including command redefinition, etc.  

To me, the complexity of this software is a clear indication that the
current model for {\LaTeX} is inappropriate for the high abstraction
level necessary to provide XML files, since it is so easy (and much
too often required) to use a \TeX-command to do something which is
irrelevant to the message the author wants to deliver.   The design of
{\LaTeX} is basically showing its age.

This is also why the \texttt{techexplorer} plugin from IBM Alphaworks
cannot implement the full functionality of {\LaTeX} -- this basically
requires the whole {\TeX} engine included.  The designers of
techexplorer chose to implement a large subset, and make do with
that.  I believe this is the right approach.  In order to be able to
use {\LaTeX} to publish documents to the Internet, a smaller subset
must be defined which is ``web safe'' and be describing content only.

A good place to start, would be the subset implemented by techexplorer.

\subsection{wv}

\myurl{http://www.wvware.com/}{wv} (originally named mswordview) is a
program which parses the data-stream inside a Word file, and generates
a set of HTML pages showing the content of the Word file.  Figures are
generated as PNG files if wv can understand them, and WMF files
otherwise.

wv generally does a good job of interpreting the content of DOC-files,
but is still lacking on the images inside, which may be rendered as
PNG files if wv can understand the description completely, otherwise
as WMF files if not, which for a long time was a severe hurdle since
most WMF conversion tools uses Windows to do the conversion to other
formats.

Fortunately,  these WMF-files can then be rendered to SVG with the
\myurl{http://www.blackdirt.com/graphics/svg/}{BlackDirt wmf2svg}
utility.  From here, the W3C
\myurl{http://www.w3.org/Graphics/SVG/Overview.htm8}{maintains a SVG
  page} including the currently known conversion utilities.


\section{Rendering HTML on the fly}

% \textsf{look at docs.sun.com - what daelen do they do?  Render SGML to
%   HTML on the fly?}

When the decision has been made to provide information in *ML on the
web server, another decision must be made.  Should the various
renderings to PDF and HTML be created before they are requested, or
when the user asks for a certain rendering?

% Table~\ref{tab:static-and-dynamic-rendering} discusses the arguments. 

% \begin{table}[htbp]
%   \begin{center}
%     \begin{tabular}{|l|p{10cm}|}
%       \hline\hline
      
%     \end{tabular}
%     \caption{Static and dynamic rendering of *XML}
%     \label{tab:static-and-dynamic-rendering}
%   \end{center}
% \end{table}
% For some reasons


% \textsf{
% Due to the rather inflexible DTD-requirements in XML, it is most
% convenient that such a tool generates stand-alone XML (\textsf{SGML ok
%   if it can be converted?  With what?}), which can be processed freely
% in the system.  This is hopefully something which gets resolved soon,
% since the validation of the tree structure in an XML document is as
% important as it being well-formed. \textsf{rewrite.}
% }
% \textsf{Move filter descriptions to Cactus chapter discussing
%   filters?  ... wordperf 9 should be ablt ot do sgml} 

% \textsf{Look for an article on static versus dynamic}

As always it depends on the nature of the data.

\textit{Static renderings} is probably suitable for documents which
rarely change their rendering or take long time to render, and which
just must be served as fast as possible.  Often there is a desire for
an inexpensive solution\footnote{``Inexpensive'' here is defined in
  terms of man hours needed to learn and maintain the web server, as
  well as the hardware needed}, and in that case a standard Apache
with a rich set of prerendered documents in a flat file system may
very well be optimal.  Personal experience has shown this to be a very
robust solution which very rarely requires human intervention.
Eric S. Raymond have reported that a modern PC with Apache is
easily capable of saturating a 10Mbps ethernet connection.

\textit{Dynamic rendering} is suitable whenever the data changes very
often, or there is a need  for the user to be able to personalize
their view of the presented documents.   Should frames be used, should
it be in text mode, should it be friendlier to blind people, should
large or small images be used etc.

For this thesis I have primarily been looking at XML-based dynamic
renderings, which at the moment is still in the development phase --
the software has not stabilized yet in order to be tuned into maximum
performance.








% \framepage{15cm}{
% The history and usage of SGML.  Creation of XML.  Describe document
% validation, and conversion (XSLT) to XML and other formats.  Freedom
% from restrictions of HTML.  Problems with tons and tons of DTD's.  The
% need for well-documented standard, robust, supported DTD's (current
% ones: TEI, ebook [buh], DocBook).  HTML conversion utilities can be
% tailored to generated DocBook XML for SSP (currently pod2docbook,
% Excel xls2xml).  Found that SGML (jade) is powerful but too slow for
% on-the-fly stuff, as opposed to XML rendererer.  Several to choose
% from if they conform to the w3c standards (DOM and SAX).   Who uses DocBook at
% the moment?  Man pages in DocBook on Solaris (look on machine).  IE50
% cannot show ``simple'' DocBook XML yet but it is the goal of that
% project.

% the xsl/docbook/contrib/outline/outline.xsl is an excellent sample of
% a small, powerful style sheet.
% }

% $Log$
% Revision 1.18  2000/05/07 16:17:45  ravn
% worked all sunday
%
% Revision 1.17  2000/05/06 12:20:19  ravn
% Editing after 14 days at home...
%
% Revision 1.16  2000/04/24 16:25:56  ravn
% final saving before printing ang going to aarhus
%
% Revision 1.15  2000/04/22 20:39:47  ravn
% tordenvejr
%
% Revision 1.14  2000/04/22 11:25:42  ravn
%
% iabout to remove Figure/Tableetc in front of \myvref
%
% Revision 1.13  2000/04/21 23:24:41  ravn
% working friday
%
% Revision 1.12  2000/04/09 14:36:44  ravn
%  WSrote section on Konsensus
%
% Revision 1.11  2000/04/07 21:10:47  ravn
%
% Opdateringer indtil 6.7 fra hjemmelaesning, og dette er praktisk taget
% afsluttet.
%
% Teksten er blevet rettet til.  Afsnittet om standarder fjernet
% (indtil videre)  og introduktionen til Let Dokumentpublicering er faldet
% [paa plads.
%
% Revision 1.10  2000/03/23 15:51:07  ravn
%
% Rearranged a lot of stuff, and starting to fill out the stuff Immerkaer
% told me.
%
% Revision 1.9  2000/03/20 13:02:50  ravn
%
% Added all updates from looktrhough at home.
%
% Revision 1.8  2000/03/18 06:06:51  ravn
% Wrote a lot on the dynam,ically generated chapter with a lot of information regarding the need for meta-information stored in the files, plus added screen shos
%
% Revision 1.7  2000/03/14 19:29:50  ravn
%
% Updated after working athome.  Major restructuring and moving stuff around.
%
% Revision 1.6  2000/03/10 05:41:37  ravn
%  wrote a lot about SQLand some about LaTeX
%
% Revision 1.5  2000/03/08 07:38:04  ravn
% Tried to remove PNG files
%
% Revision 1.4  2000/03/08 06:51:12  ravn
% Edited a lot of text in XML
%
% Revision 1.3  2000/03/05 02:32:16  ravn
% Added lots of pictures with relatively little dialoue.
%
% Making progress.
%
% Revision 1.2  2000/03/04 21:39:15  ravn
%
%
% Added a lot of information.  Prepared for screendumps.
%
% Revision 1.1.1.1  2000/03/02 21:55:31  ravn
% Speciale files
%
% Revision 1.3  2000/02/26 22:07:33  ravn
% Filled stuff on the various XSLT processors and described the findings hereof.
%
% Revision 1.2  2000/02/26 15:57:39  ravn
% Added a lot of text at the latter part.
%
% Revision 1.1  2000/02/23 02:23:26  ravn
% Initial revision
%
%

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "rapport"
%%% End: 
